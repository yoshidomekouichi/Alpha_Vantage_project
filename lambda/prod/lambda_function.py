#!/usr/bin/env python3
"""
AWS Lambda Function for Daily Stock Data Fetching

This Lambda function fetches the latest daily stock data from Alpha Vantage API
and stores it in AWS S3. It is designed to be triggered on a schedule.
"""

import os
import sys
import json
import logging
import time
from datetime import datetime, timedelta
import traceback

# ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å‡¦ç†ç”¨
try:
    # Python 3.9ä»¥é™ã®æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
    from zoneinfo import ZoneInfo
    def get_jst_time():
        return datetime.now(ZoneInfo("Asia/Tokyo"))
except ImportError:
    # Python 3.9æœªæº€ã¾ãŸã¯zoneinfoéå¯¾å¿œç’°å¢ƒç”¨
    try:
        import pytz
        def get_jst_time():
            return datetime.now(pytz.timezone('Asia/Tokyo'))
    except ImportError:
        # pytzã‚‚åˆ©ç”¨ã§ããªã„å ´åˆã¯ã€UTCã«9æ™‚é–“ã‚’åŠ ç®—
        def get_jst_time():
            return datetime.now() + timedelta(hours=9)

# Lambdaç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
os.environ['AWS_LAMBDA_EXECUTION'] = 'true'

# ãƒ­ã‚¬ãƒ¼åˆæœŸåŒ–
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# srcãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append('.')
sys.path.append('/var/task')  # Lambdaç’°å¢ƒã§ã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’é…å»¶ã•ã›ã‚‹ï¼ˆLambdaå®Ÿè¡Œæ™‚ã«å¿…è¦ãªã‚‚ã®ã ã‘ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼‰
try:
    from utils.alerts import AlertManager
except ImportError:
    # Lambdaç’°å¢ƒã§ã¯ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦ã¿ã‚‹
    try:
        from alerts import AlertManager
    except ImportError:
        logger.error("AlertManagerã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")

def setup_logging():
    """
    Set up logging configuration.
    
    Returns:
        Logger object
    """
    # ãƒ­ã‚¬ãƒ¼åˆæœŸåŒ–
    logger = logging.getLogger(__name__)
    
    # Lambdaç’°å¢ƒã§ã¯ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’/tmpä»¥ä¸‹ã«è¨­å®š
    log_dir = '/tmp/logs'
    
    # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
    try:
        os.makedirs(log_dir, exist_ok=True)
        logger.info(f"ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ: {log_dir}")
    except Exception as e:
        logger.error(f"ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logger.error(traceback.format_exc())
    
    # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¨­å®š
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    
    # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã‚’è¨­å®š
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    
    # ãƒ­ã‚¬ãƒ¼ã«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ 
    logger.addHandler(console_handler)
    
    return logger

def lambda_handler(event, context):
    """
    AWS Lambda handler function.
    
    Args:
        event: Lambda event data
        context: Lambda context
        
    Returns:
        Dictionary with execution results
    """
    start_time = time.time()
    
    # ãƒ­ã‚®ãƒ³ã‚°ã®è¨­å®š
    logger = setup_logging()
    
    # ç’°å¢ƒå¤‰æ•°ã‚’å–å¾—
    api_key = os.environ.get('ALPHA_VANTAGE_API_KEY', 'demo')
    s3_bucket = os.environ.get('S3_BUCKET', 'Not set')
    region = os.environ.get('REGION', 'ap-northeast-1')
    stock_symbols = os.environ.get('STOCK_SYMBOLS', 'NVDA,AAPL,MSFT')
    mock_mode = os.environ.get('MOCK_MODE', 'false').lower() == 'true'
    debug_mode = os.environ.get('DEBUG_MODE', 'false').lower() == 'true'
    
    # Slacké€šçŸ¥ã®è¨­å®šã‚’å–å¾—
    slack_enabled = os.environ.get('SLACK_ENABLED', 'false').lower() == 'true'
    slack_webhook_url = os.environ.get('SLACK_WEBHOOK_URL', '')
    slack_webhook_url_error = os.environ.get('SLACK_WEBHOOK_URL_ERROR', slack_webhook_url)
    slack_webhook_url_warning = os.environ.get('SLACK_WEBHOOK_URL_WARNING', slack_webhook_url)
    slack_webhook_url_info = os.environ.get('SLACK_WEBHOOK_URL_INFO', slack_webhook_url)
    
    # AlertManagerã®åˆæœŸåŒ–
    alert_manager = None
    if slack_enabled:
        try:
            alert_manager = AlertManager(
                None,  # email_config
                slack_webhook_url_error,
                slack_webhook_url_warning,
                slack_webhook_url_info
            )
            alert_manager.set_logger(logger)
            logger.info("AlertManagerã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
            
            # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯Slackè¨­å®šã®è©³ç´°ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
            if debug_mode:
                logger.debug("=" * 80)
                logger.debug("Slackè¨­å®šã®è©³ç´°:")
                logger.debug(f"slack_enabled: {slack_enabled}")
                logger.debug(f"slack_webhook_url: {slack_webhook_url}")
                logger.debug(f"slack_webhook_url_error: {slack_webhook_url_error}")
                logger.debug(f"slack_webhook_url_warning: {slack_webhook_url_warning}")
                logger.debug(f"slack_webhook_url_info: {slack_webhook_url_info}")
                logger.debug("=" * 80)
        except Exception as e:
            logger.error(f"AlertManagerã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            logger.error(traceback.format_exc())
    
    # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‚’è¨­å®š
    env_type = "Lambda"
    if mock_mode:
        env_type += " (Mock)"
    
    # å®Ÿè¡Œé–‹å§‹ãƒ­ã‚°
    logger.info("=" * 80)
    logger.info(f"ğŸš€ Starting Lambda function for daily stock data fetch at {datetime.now().isoformat()}")
    logger.info(f"ğŸ”§ Environment: {env_type}")
    logger.info(f"ğŸ”‘ API Key: {'*' * (len(api_key) - 4) + api_key[-4:] if len(api_key) > 4 else '****'}")
    logger.info(f"ğŸª£ S3 Bucket: {s3_bucket}")
    logger.info(f"ğŸŒ Region: {region}")
    logger.info(f"ğŸ“Š Stock Symbols: {stock_symbols}")
    logger.info(f"ğŸ› Debug Mode: {debug_mode}")
    logger.info("=" * 80)
    
    try:
        # ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        if mock_mode:
            logger.info("ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œä¸­ã§ã™ã€‚ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã—ã¾ã™ã€‚")
            
            # å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—
            end_time = time.time()
            execution_time = end_time - start_time
            
            return {
                'statusCode': 200,
                'body': json.dumps({
                    'success': True,
                    'execution_time': f"{execution_time:.2f} seconds",
                    'message': 'Mock mode execution successful',
                    'data': {
                        'symbols': stock_symbols.split(','),
                        'timestamp': datetime.now().isoformat()
                    }
                })
            }
        
        # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å–å¾—å‡¦ç†
        logger.info("Alpha Vantage APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™...")
        
        # ã“ã“ã§å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å–å¾—å‡¦ç†ã‚’è¡Œã†
        # æ³¨æ„: ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¯ã“ã“ã§è¡Œã†
        try:
            import boto3
            from botocore.exceptions import ClientError
            
            # S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
            s3_client = boto3.client('s3', region_name=region)
            
            # æ ªå¼ã‚·ãƒ³ãƒœãƒ«ã®ãƒªã‚¹ãƒˆ
            symbols = stock_symbols.split(',')
            
            # å„ã‚·ãƒ³ãƒœãƒ«ã«å¯¾ã—ã¦å‡¦ç†ã‚’å®Ÿè¡Œ
            results = {}
            error_details = {}  # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’è¨˜éŒ²ã™ã‚‹è¾æ›¸
            data_sizes = {}     # å„ã‚·ãƒ³ãƒœãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’è¨˜éŒ²ã™ã‚‹è¾æ›¸
            s3_paths = {}       # å„ã‚·ãƒ³ãƒœãƒ«ã®S3ãƒ‘ã‚¹ã‚’è¨˜éŒ²ã™ã‚‹è¾æ›¸
            symbol_dates = {}   # å„ã‚·ãƒ³ãƒœãƒ«ã®æ—¥ä»˜ã‚’è¨˜éŒ²ã™ã‚‹è¾æ›¸
            latest_date = None  # æœ€æ–°ã®æ—¥ä»˜ï¼ˆã™ã¹ã¦ã®ã‚·ãƒ³ãƒœãƒ«ã§å…±é€šï¼‰
            for symbol in symbols:
                logger.info(f"ã‚·ãƒ³ãƒœãƒ« {symbol} ã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
                
                # Alpha Vantage APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                try:
                    import requests
                    
                    # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆURLã‚’æ§‹ç¯‰
                    base_url = "https://www.alphavantage.co/query"
                    params = {
                        "function": "TIME_SERIES_DAILY",
                        "symbol": symbol,
                        "apikey": api_key,
                        "outputsize": "compact"  # æœ€æ–°ã®100ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                    }
                    
                    # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
                    logger.info(f"Alpha Vantage APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã—ã¾ã™: {base_url}?function={params['function']}&symbol={params['symbol']}&outputsize={params['outputsize']}")
                    response = requests.get(base_url, params=params)
                    
                    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç¢ºèª
                    if response.status_code != 200:
                        error_msg = f"APIãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸ: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}"
                        logger.error(error_msg)
                        logger.error(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.text}")
                        results[symbol] = 'error'
                        error_details[symbol] = {
                            'error_type': 'API Request Error',
                            'status_code': response.status_code,
                            'message': error_msg,
                            'response': response.text[:200] + '...' if len(response.text) > 200 else response.text
                        }
                        continue
                    
                    # JSONãƒ‡ãƒ¼ã‚¿ã‚’è§£æ
                    data = response.json()
                    
                    # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
                    if "Error Message" in data:
                        error_msg = f"APIã‚¨ãƒ©ãƒ¼: {data['Error Message']}"
                        logger.error(error_msg)
                        results[symbol] = 'error'
                        error_details[symbol] = {
                            'error_type': 'API Error',
                            'message': error_msg,
                            'api_error': data['Error Message']
                        }
                        continue
                    
                    if "Time Series (Daily)" not in data:
                        error_msg = f"äºˆæœŸã—ãªã„APIãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼"
                        logger.error(f"{error_msg}: {data}")
                        results[symbol] = 'error'
                        error_details[symbol] = {
                            'error_type': 'Unexpected Response Format',
                            'message': error_msg,
                            'response_keys': list(data.keys())
                        }
                        continue
                    
                    # æœ€æ–°ã®æ—¥ä»˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                    time_series = data["Time Series (Daily)"]
                    latest_date = list(time_series.keys())[0]  # æœ€åˆã®ã‚­ãƒ¼ãŒæœ€æ–°ã®æ—¥ä»˜
                    latest_data = time_series[latest_date]
                    
                    # å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                    stock_data = {
                        'symbol': symbol,
                        'timestamp': datetime.now().isoformat(),
                        'date': latest_date,
                        'open': float(latest_data['1. open']),
                        'high': float(latest_data['2. high']),
                        'low': float(latest_data['3. low']),
                        'close': float(latest_data['4. close']),
                        'volume': int(latest_data['5. volume'])
                    }
                    
                    logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ: {symbol} ({latest_date})")
                    
                except Exception as e:
                    error_msg = f"ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
                    logger.error(error_msg)
                    logger.error(traceback.format_exc())
                    results[symbol] = 'error'
                    error_details[symbol] = {
                        'error_type': 'Data Fetch Error',
                        'message': error_msg,
                        'symbol': symbol,
                        'error': str(e)
                    }
                    continue
                
                # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
                json_data = json.dumps(stock_data)
                data_size = len(json_data)
                data_sizes[symbol] = data_size
                
                # å‰å›ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦æ¯”è¼ƒã™ã‚‹ãŸã‚ã®æº–å‚™
                previous_data = None
                previous_size = 0
                size_diff_percent = 0
                
                try:
                    # å‰å›ã®ãƒ‡ãƒ¼ã‚¿ã‚’S3ã‹ã‚‰èª­ã¿è¾¼ã‚€è©¦ã¿
                    try:
                        # utils/s3_paths.pyã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
                        try:
                            from utils.s3_paths import get_s3_key
                        except ImportError:
                            # Lambdaç’°å¢ƒã§ã¯ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦ã¿ã‚‹
                            from s3_paths import get_s3_key
                        
                        # ç’°å¢ƒè¨­å®šã‚’å–å¾—
                        is_mock = os.environ.get('MOCK_MODE', 'false').lower() == 'true'
                        
                        # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®S3ã‚­ãƒ¼ã‚’ç”Ÿæˆ
                        latest_key = get_s3_key(
                            symbol=symbol,
                            data_type='raw',
                            is_latest=True,
                            is_mock=is_mock
                        )
                    except ImportError:
                        # s3_paths.pyãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æ—§å½¢å¼ã‚’ä½¿ç”¨
                        latest_key = f"{symbol}/latest.json"
                    
                    # å‰å›ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                    try:
                        response = s3_client.get_object(Bucket=s3_bucket, Key=latest_key)
                        previous_data = json.loads(response['Body'].read().decode('utf-8'))
                        previous_size = len(json.dumps(previous_data))
                        
                        # ã‚µã‚¤ã‚ºã®å·®åˆ†ã‚’è¨ˆç®—ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆï¼‰
                        if previous_size > 0:
                            size_diff_percent = ((data_size - previous_size) / previous_size) * 100
                        
                        logger.info(f"å‰å›ã®ãƒ‡ãƒ¼ã‚¿ã¨æ¯”è¼ƒ: ç¾åœ¨={data_size}ãƒã‚¤ãƒˆ, å‰å›={previous_size}ãƒã‚¤ãƒˆ, å·®åˆ†={size_diff_percent:.2f}%")
                    except Exception as e:
                        logger.warning(f"å‰å›ã®ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆåˆå›å®Ÿè¡Œã®å¯èƒ½æ€§ã‚ã‚Šï¼‰: {e}")
                except Exception as e:
                    logger.warning(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºæ¯”è¼ƒå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                
                # å„ã‚·ãƒ³ãƒœãƒ«ã®æ—¥ä»˜ã‚’ä¿å­˜
                symbol_date = stock_data['date']
                symbol_dates[symbol] = symbol_date
                
                # S3ã«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                try:
                    # æ–°ã—ã„S3ãƒ‘ã‚¹æ§‹é€ ã‚’ä½¿ç”¨
                    try:
                        # utils/s3_paths.pyã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
                        try:
                            from utils.s3_paths import get_s3_key, get_metadata_key
                        except ImportError:
                            # Lambdaç’°å¢ƒã§ã¯ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦ã¿ã‚‹
                            from s3_paths import get_s3_key, get_metadata_key
                        
                        # ç’°å¢ƒè¨­å®šã‚’å–å¾—
                        is_mock = os.environ.get('MOCK_MODE', 'false').lower() == 'true'
                        
                        # æ¨™æº–éšå±¤æ§‹é€ (V2)ã®S3ã‚­ãƒ¼ã‚’ç”Ÿæˆ
                        # 1. æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ç”¨ã®ã‚­ãƒ¼
                        # å„ã‚·ãƒ³ãƒœãƒ«ã”ã¨ã«APIã‹ã‚‰å–å¾—ã—ãŸæ—¥ä»˜ã‚’ä½¿ç”¨
                        symbol_date = stock_data['date']  # APIã‹ã‚‰å–å¾—ã—ãŸæ—¥ä»˜
                        daily_key = get_s3_key(
                            symbol=symbol,
                            data_type='raw',
                            date=symbol_date,  # APIã‹ã‚‰å–å¾—ã—ãŸæ—¥ä»˜ã‚’ä½¿ç”¨
                            is_latest=False,
                            is_mock=is_mock
                        )
                        
                        # 2. æœ€æ–°ãƒ‡ãƒ¼ã‚¿ç”¨ã®ã‚­ãƒ¼
                        latest_key = get_s3_key(
                            symbol=symbol,
                            data_type='raw',
                            is_latest=True,
                            is_mock=is_mock
                        )
                        
                        # 3. å…¨æœŸé–“ãƒ‡ãƒ¼ã‚¿ç”¨ã®ã‚­ãƒ¼ï¼ˆä»Šå›ã¯æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã¨åŒã˜ï¼‰
                        full_key = get_s3_key(
                            symbol=symbol,
                            data_type='raw',
                            is_mock=is_mock
                        )
                        
                        # 4. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”¨ã®ã‚­ãƒ¼
                        metadata_key = get_metadata_key(
                            symbol=symbol,
                            data_type='raw',
                            is_mock=is_mock
                        )
                    except ImportError:
                        # s3_paths.pyãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æ—§å½¢å¼ã‚’ä½¿ç”¨
                        logger.warning("s3_paths.pyãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€æ—§å½¢å¼ã®S3ãƒ‘ã‚¹ã‚’ä½¿ç”¨ã—ã¾ã™")
                        daily_key = f"daily/{symbol}/{symbol_date}.json"  # APIã‹ã‚‰å–å¾—ã—ãŸæ—¥ä»˜ã‚’ä½¿ç”¨
                        latest_key = f"{symbol}/latest.json"
                        full_key = f"{symbol}/full.json"
                        metadata_key = f"{symbol}/metadata.json"
                    
                    # ãƒ¡ã‚¤ãƒ³ã®S3ãƒ‘ã‚¹ã‚’è¨­å®šï¼ˆæ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ï¼‰
                    s3_path = f"s3://{s3_bucket}/{daily_key}"
                    s3_paths[symbol] = s3_path
                    
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
                    metadata = {
                        'symbol': symbol,
                        'last_updated': datetime.now().isoformat(),
                        'latest_date': symbol_date,  # å„ã‚·ãƒ³ãƒœãƒ«ã®æ—¥ä»˜ã‚’ä½¿ç”¨
                        'data_points': 1,  # æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã®ã¿ã®å ´åˆ
                        'date_range': {
                            'start': symbol_date,  # å„ã‚·ãƒ³ãƒœãƒ«ã®æ—¥ä»˜ã‚’ä½¿ç”¨
                            'end': symbol_date     # å„ã‚·ãƒ³ãƒœãƒ«ã®æ—¥ä»˜ã‚’ä½¿ç”¨
                        }
                    }
                    
                    # 1. æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
                    s3_client.put_object(
                        Bucket=s3_bucket,
                        Key=daily_key,
                        Body=json_data,
                        ContentType='application/json'
                    )
                    logger.info(f"æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’S3ã«ä¿å­˜ã—ã¾ã—ãŸ: s3://{s3_bucket}/{daily_key}")
                    
                    # 2. æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
                    s3_client.put_object(
                        Bucket=s3_bucket,
                        Key=latest_key,
                        Body=json_data,
                        ContentType='application/json'
                    )
                    logger.info(f"æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’S3ã«ä¿å­˜ã—ã¾ã—ãŸ: s3://{s3_bucket}/{latest_key}")
                    
                    # 3. å…¨æœŸé–“ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ï¼ˆä»Šå›ã¯æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã¨åŒã˜ï¼‰
                    s3_client.put_object(
                        Bucket=s3_bucket,
                        Key=full_key,
                        Body=json_data,
                        ContentType='application/json'
                    )
                    logger.info(f"å…¨æœŸé–“ãƒ‡ãƒ¼ã‚¿ã‚’S3ã«ä¿å­˜ã—ã¾ã—ãŸ: s3://{s3_bucket}/{full_key}")
                    
                    # 4. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
                    s3_client.put_object(
                        Bucket=s3_bucket,
                        Key=metadata_key,
                        Body=json.dumps(metadata),
                        ContentType='application/json'
                    )
                    logger.info(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’S3ã«ä¿å­˜ã—ã¾ã—ãŸ: s3://{s3_bucket}/{metadata_key}")
                    
                    results[symbol] = 'success'
                except ClientError as e:
                    error_msg = f"S3ã¸ã®ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"
                    logger.error(error_msg)
                    results[symbol] = 'error'
                    error_details[symbol] = {
                        'error_type': 'S3 Storage Error',
                        'message': error_msg,
                        's3_bucket': s3_bucket,
                        's3_key': s3_key,
                        'error': str(e)
                    }
                except Exception as e:
                    error_msg = f"S3ã¸ã®ãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­ã«äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
                    logger.error(error_msg)
                    logger.error(traceback.format_exc())
                    results[symbol] = 'error'
                    error_details[symbol] = {
                        'error_type': 'Unexpected S3 Error',
                        'message': error_msg,
                        'error': str(e)
                    }
            
            # å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—
            end_time = time.time()
            execution_time = end_time - start_time
            
            # çµæœã‚’é›†è¨ˆ
            success_count = sum(1 for result in results.values() if result == 'success')
            failure_count = sum(1 for result in results.values() if result != 'success')
            
            # Slacké€šçŸ¥ã‚’é€ä¿¡
            if slack_enabled and alert_manager:
                try:
                    # å®Ÿè¡Œç’°å¢ƒæƒ…å ±ã‚’å–å¾—
                    env_info = f"Environment: {env_type}"
                    utc_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    jst_timestamp = get_jst_time().strftime("%Y-%m-%d %H:%M:%S")
                    
                    # å…±é€šã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                    common_fields = [
                        {"title": "Environment", "value": env_type, "short": True},
                        {"title": "Execution Time", "value": f"{execution_time:.2f} seconds", "short": True},
                        {"title": "Timestamp (UTC)", "value": utc_timestamp, "short": True},
                        {"title": "Timestamp (JST)", "value": jst_timestamp, "short": True},
                    ]
                    
                    # çµæœã«åŸºã¥ã„ã¦é€šçŸ¥ã‚’é€ä¿¡
                    if failure_count > 0:
                        # å¤±æ•—ã—ãŸã‚·ãƒ³ãƒœãƒ«ã‚’æŠ½å‡º
                        failed_symbols = [symbol for symbol, result in results.items() if result != 'success']
                        
                        # å¤±æ•—æƒ…å ±ã‚’è©³ç´°ã«å«ã‚ã‚‹
                        failure_fields = [
                            {"title": "Failed Symbols", "value": ", ".join(failed_symbols), "short": False},
                            {"title": "Success Count", "value": str(success_count), "short": True},
                            {"title": "Failure Count", "value": str(failure_count), "short": True}
                        ]
                        
                        # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°æƒ…å ±ã‚’è¿½åŠ 
                        for symbol in failed_symbols:
                            if symbol in error_details:
                                error_info = error_details[symbol]
                                failure_fields.append({
                                    "title": f"Error Details for {symbol}",
                                    "value": f"Type: {error_info.get('error_type', 'Unknown')}\nMessage: {error_info.get('message', 'No message')}",
                                    "short": False
                                })
                        
                        # è©³ç´°ãªçµæœæƒ…å ±
                        detailed_results = "\n".join([f"{symbol}: {result}" for symbol, result in results.items()])
                        
                        # è­¦å‘Šã‚¢ãƒ©ãƒ¼ãƒˆã‚’é€ä¿¡
                        alert_message = f"âš ï¸ Lambda: Daily stock data fetch completed with {failure_count} failures"
                        alert_details = f"""
WARNING: Some stock data fetch operations failed.

Execution time: {execution_time:.2f} seconds
Environment: {env_type}
Successful: {success_count}
Failed: {failure_count}

Failed symbols: {', '.join(failed_symbols)}

Detailed results:
{detailed_results}
"""
                        alert_manager.send_warning_alert(
                            alert_message,
                            alert_details,
                            source="lambda_function.py",
                            send_email=False,
                            send_slack=True,
                            additional_fields=common_fields + failure_fields
                        )
                        logger.info("âœ… è­¦å‘Šé€šçŸ¥ã‚’é€ä¿¡ã—ã¾ã—ãŸ")
                    else:
                        # æˆåŠŸã—ãŸã‚·ãƒ³ãƒœãƒ«ã‚’æŠ½å‡º
                        successful_symbols = [symbol for symbol, result in results.items() if result == 'success']
                        
                        # ãƒ‡ãƒ¼ã‚¿æ—¥ä»˜ã®æƒ…å ±
                        # å„ã‚·ãƒ³ãƒœãƒ«ã®æ—¥ä»˜ã‚’è¡¨ç¤º
                        date_info = ", ".join([f"{symbol}: {date}" for symbol, date in symbol_dates.items()])
                        data_date_field = {"title": "Data Dates", "value": date_info, "short": False}
                        
                        # ãƒ‡ãƒ¼ã‚¿æƒ…å ±ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½œæˆ
                        total_data_size = sum(data_sizes.values())
                        avg_data_size = total_data_size / len(data_sizes) if data_sizes else 0
                        total_files = success_count * 4  # å„ã‚·ãƒ³ãƒœãƒ«ã«ã¤ã4ç¨®é¡ã®ãƒ•ã‚¡ã‚¤ãƒ«
                        
                        # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã®ã‚µã‚¤ã‚ºã‚’è¨ˆç®—ï¼ˆä»Šå›ã¯å…¨ã¦åŒã˜ï¼‰
                        daily_size = total_data_size
                        latest_size = total_data_size
                        full_size = total_data_size
                        metadata_size = len(json.dumps(metadata)) * success_count if success_count > 0 else 0
                        
                        # å‰å›ã¨ã®æ¯”è¼ƒæƒ…å ±ã‚’å–å¾—
                        size_diff_info = ""
                        for symbol, size in data_sizes.items():
                            try:
                                # å‰å›ã®ãƒ‡ãƒ¼ã‚¿ã‚’S3ã‹ã‚‰èª­ã¿è¾¼ã‚€è©¦ã¿
                                try:
                                    # utils/s3_paths.pyã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
                                    try:
                                        from utils.s3_paths import get_s3_key
                                    except ImportError:
                                        # Lambdaç’°å¢ƒã§ã¯ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦ã¿ã‚‹
                                        from s3_paths import get_s3_key
                                    
                                    # ç’°å¢ƒè¨­å®šã‚’å–å¾—
                                    is_mock = os.environ.get('MOCK_MODE', 'false').lower() == 'true'
                                    
                                    # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®S3ã‚­ãƒ¼ã‚’ç”Ÿæˆ
                                    latest_key = get_s3_key(
                                        symbol=symbol,
                                        data_type='raw',
                                        is_latest=True,
                                        is_mock=is_mock
                                    )
                                except ImportError:
                                    # s3_paths.pyãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æ—§å½¢å¼ã‚’ä½¿ç”¨
                                    latest_key = f"{symbol}/latest.json"
                                
                                # å‰å›ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                                response = s3_client.get_object(Bucket=s3_bucket, Key=latest_key)
                                previous_data = json.loads(response['Body'].read().decode('utf-8'))
                                previous_size = len(json.dumps(previous_data))
                                
                                # ã‚µã‚¤ã‚ºã®å·®åˆ†ã‚’è¨ˆç®—ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆï¼‰
                                if previous_size > 0:
                                    size_diff_percent = ((size - previous_size) / previous_size) * 100
                                    size_diff_info += f"- {symbol}: {size_diff_percent:+.2f}% ({previous_size} â†’ {size} ãƒã‚¤ãƒˆ)\n"
                            except Exception as e:
                                size_diff_info += f"- {symbol}: å‰å›ãƒ‡ãƒ¼ã‚¿ãªã—ï¼ˆåˆå›å®Ÿè¡Œï¼‰\n"
                        
                        # ãƒ‡ãƒ¼ã‚¿æƒ…å ±ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½œæˆ
                        data_info_field = {
                            "title": "Data Information", 
                            "value": f"- å‡¦ç†ã—ãŸã‚·ãƒ³ãƒœãƒ«æ•°: {success_count}\n"
                                    f"- æ›´æ–°ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {success_count}Ã—4 = {total_files} ãƒ•ã‚¡ã‚¤ãƒ«\n"
                                    f"- ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: åˆè¨ˆ {total_data_size / 1024:.2f} KB (å¹³å‡ {avg_data_size / 1024:.2f} KB/ã‚·ãƒ³ãƒœãƒ«)\n"
                                    f"- ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã‚µã‚¤ã‚º:\n"
                                    f"  â€¢ æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿: {daily_size / 1024:.2f} KB\n"
                                    f"  â€¢ æœ€æ–°ãƒ‡ãƒ¼ã‚¿: {latest_size / 1024:.2f} KB\n"
                                    f"  â€¢ å…¨æœŸé–“ãƒ‡ãƒ¼ã‚¿: {full_size / 1024:.2f} KB\n"
                                    f"  â€¢ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {metadata_size / 1024:.2f} KB",
                            "short": False
                        }
                        
                        # å‰å›ã¨ã®æ¯”è¼ƒæƒ…å ±ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½œæˆ
                        data_change_field = {
                            "title": "Data Change Information",
                            "value": size_diff_info if size_diff_info else "- å‰å›ãƒ‡ãƒ¼ã‚¿ãªã—ï¼ˆåˆå›å®Ÿè¡Œï¼‰",
                            "short": False
                        }
                        
                        # S3ãƒ‘ã‚¹æƒ…å ±ã‚’è¿½åŠ ï¼ˆä»£è¡¨çš„ãªãƒ‘ã‚¹ã¨æ®‹ã‚Šã®ä»¶æ•°ï¼‰
                        s3_path_examples = []
                        if successful_symbols:
                            # æœ€åˆã®3ã¤ã®ã‚·ãƒ³ãƒœãƒ«ï¼ˆã¾ãŸã¯å…¨éƒ¨ï¼‰ã®ãƒ‘ã‚¹ã‚’è¡¨ç¤º
                            display_count = min(3, len(successful_symbols))
                            for i in range(display_count):
                                symbol = successful_symbols[i]
                                # æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹ã‚’è¡¨ç¤º
                                s3_path_examples.append(f"{symbol}: {s3_paths[symbol]}")
                            
                            # æ®‹ã‚Šã®ä»¶æ•°ãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤º
                            remaining = len(successful_symbols) - display_count
                            if remaining > 0:
                                s3_path_examples.append(f"... ä»– {remaining} ä»¶")
                        
                        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã®ã‚µãƒãƒªãƒ¼æƒ…å ±ã‚’è¿½åŠ 
                        file_types_summary = "å„ã‚·ãƒ³ãƒœãƒ«ã«ã¤ãä»¥ä¸‹ã®4ç¨®é¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã—ã¾ã—ãŸï¼š\n"
                        file_types_summary += "- æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿: .../daily/{year}/{month}/{day}.json\n"
                        file_types_summary += "- æœ€æ–°ãƒ‡ãƒ¼ã‚¿: .../latest.json\n"
                        file_types_summary += "- å…¨æœŸé–“ãƒ‡ãƒ¼ã‚¿: .../full.json\n"
                        file_types_summary += "- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: .../metadata.json"
                        
                        s3_paths_field = {
                            "title": "S3 Storage Paths", 
                            "value": file_types_summary + "\n\n" + "\n".join(s3_path_examples), 
                            "short": False
                        }
                        
                        # æˆåŠŸæƒ…å ±ã‚’è©³ç´°ã«å«ã‚ã‚‹
                        success_fields = [
                            {"title": "Successful Symbols", "value": ", ".join(successful_symbols), "short": False},
                            {"title": "Total Successful", "value": str(success_count), "short": True},
                            data_date_field,
                            data_info_field,
                            data_change_field,
                            s3_paths_field
                        ]
                        
                        # æˆåŠŸã‚¢ãƒ©ãƒ¼ãƒˆã‚’é€ä¿¡
                        alert_message = f"âœ… Lambda: Daily stock data fetch completed successfully for all {success_count} symbols"
                        
                        # å„ã‚·ãƒ³ãƒœãƒ«ã®æ—¥ä»˜æƒ…å ±ã‚’è©³ç´°ã«å«ã‚ã‚‹
                        date_details = "\n".join([f"- {symbol}: {date}" for symbol, date in symbol_dates.items()])
                        
                        alert_details = f"""
INFO: Stock data fetch summary.

Execution time: {execution_time:.2f} seconds
Environment: {env_type}
Successful symbols: {', '.join(successful_symbols)}
Total successful: {success_count}
Total data size: {total_data_size / 1024:.2f} KB

Data dates:
{date_details}
"""
                        alert_manager.send_success_alert(
                            alert_message,
                            alert_details,
                            source="lambda_function.py",
                            send_email=False,
                            send_slack=True,
                            additional_fields=common_fields + success_fields
                        )
                        logger.info("âœ… æˆåŠŸé€šçŸ¥ã‚’é€ä¿¡ã—ã¾ã—ãŸ")
                except Exception as e:
                    logger.error(f"âŒ Slacké€šçŸ¥å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    logger.error(traceback.format_exc())
            elif slack_enabled:
                logger.warning("AlertManagerãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€Slacké€šçŸ¥ã‚’é€ä¿¡ã§ãã¾ã›ã‚“")
            else:
                logger.info("Slacké€šçŸ¥ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
            
            return {
                'statusCode': 200,
                'body': json.dumps({
                    'success': True,
                    'execution_time': f"{execution_time:.2f} seconds",
                    'results': results,
                    'success_count': success_count,
                    'failure_count': failure_count
                })
            }
        except ImportError as e:
            logger.error(f"ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            logger.error(traceback.format_exc())
            raise
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
        end_time = time.time()
        execution_time = end_time - start_time
        
        logger.error(f"âŒ Lambdaé–¢æ•°ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logger.error(traceback.format_exc())
        
        # ã‚¨ãƒ©ãƒ¼é€šçŸ¥ã‚’Slackã«é€ä¿¡
        if slack_enabled and alert_manager:
            try:
                # å®Ÿè¡Œç’°å¢ƒæƒ…å ±ã‚’å–å¾—
                env_info = f"Environment: {env_type}"
                utc_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                jst_timestamp = get_jst_time().strftime("%Y-%m-%d %H:%M:%S")
                
                # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è©³ç´°ã«å«ã‚ã‚‹
                error_fields = [
                    {"title": "Environment", "value": env_type, "short": True},
                    {"title": "Execution Time", "value": f"{execution_time:.2f} seconds", "short": True},
                    {"title": "Timestamp (UTC)", "value": utc_timestamp, "short": True},
                    {"title": "Timestamp (JST)", "value": jst_timestamp, "short": True},
                    {"title": "Error", "value": str(e), "short": False}
                ]
                
                # ã‚¨ãƒ©ãƒ¼ã‚¢ãƒ©ãƒ¼ãƒˆã‚’é€ä¿¡
                alert_message = f"âŒ Lambda: Daily stock data fetch failed with error"
                alert_details = f"""
ERROR: Lambda function execution failed.

Execution time: {execution_time:.2f} seconds
Environment: {env_type}
Error: {str(e)}

Stack trace:
{traceback.format_exc()}
"""
                alert_manager.send_error_alert(
                    alert_message,
                    alert_details,
                    source="lambda_function.py",
                    send_email=False,
                    send_slack=True,
                    additional_fields=error_fields
                )
                logger.info("âœ… ã‚¨ãƒ©ãƒ¼é€šçŸ¥ã‚’é€ä¿¡ã—ã¾ã—ãŸ")
            except Exception as notify_error:
                logger.error(f"âŒ Slacké€šçŸ¥å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {notify_error}")
                logger.error(traceback.format_exc())
        
        # Lambdaé–¢æ•°ã®æˆ»ã‚Šå€¤ï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰
        return {
            'statusCode': 500,
            'body': json.dumps({
                'success': False,
                'execution_time': f"{execution_time:.2f} seconds",
                'error': str(e)
            })
        }

# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆç”¨
if __name__ == "__main__":
    try:
        # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚¤ãƒ™ãƒ³ãƒˆã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        test_event = {}
        test_context = None
        
        # Lambdaé–¢æ•°ã‚’å®Ÿè¡Œ
        result = lambda_handler(test_event, test_context)
        print(f"Lambda function result: {json.dumps(result, indent=2)}")
    except Exception as e:
        # Catch any unexpected exceptions
        print(f"âŒ Critical error: {e}")
        traceback.print_exc()
        sys.exit(1)
