#!/usr/bin/env python3
"""
Daily Stock Data Fetcher

This script fetches the latest daily stock data from Alpha Vantage API
and stores it in AWS S3. It is designed to be run as a daily batch job.
"""

import os
import sys
import time
import json
import logging
from datetime import datetime
from pathlib import Path
import traceback

try:
    from src.config import Config
    from src.utils.api_client import AlphaVantageClient
    from src.utils.data_processing import StockDataProcessor
    from src.utils.storage import S3Storage
    from src.utils.atomic_s3 import AtomicS3
    from src.utils.logging_utils import LoggerManager, log_execution_time
    from src.utils.alerts import AlertManager
except ImportError:
    # Lambda ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç’°å¢ƒç”¨ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    from config import Config
    from utils.api_client import AlphaVantageClient
    from utils.data_processing import StockDataProcessor
    from utils.storage import S3Storage
    from utils.atomic_s3 import AtomicS3
    from utils.logging_utils import LoggerManager, log_execution_time
    from utils.alerts import AlertManager

def setup_components(config):
    """
    Set up all components with the given configuration.
    
    Args:
        config: Configuration object
        
    Returns:
        Tuple of (logger, api_client, data_processor, s3_storage, atomic_s3, alert_manager)
    """
    # ãƒ­ã‚¬ãƒ¼åˆæœŸåŒ–
    logger = logging.getLogger(__name__)
    
    try:
        # Set up logger
        log_level = getattr(logging, config.log_level.upper(), logging.INFO)
        
        # Lambdaç’°å¢ƒã®å ´åˆã¯ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’/tmpä»¥ä¸‹ã«è¨­å®š
        if os.environ.get('AWS_LAMBDA_EXECUTION', '').lower() == 'true':
            log_dir = Path('/tmp/logs')
            logger.info(f"Lambdaç’°å¢ƒã®ãŸã‚ã€ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ {log_dir} ã«è¨­å®šã—ã¾ã™")
        else:
            log_dir = config.log_dir
            logger.info(f"ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®ãŸã‚ã€ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ {log_dir} ã«è¨­å®šã—ã¾ã™")
            
        # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        try:
            os.makedirs(log_dir, exist_ok=True)
            logger.info(f"ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ: {log_dir}")
        except Exception as e:
            logger.error(f"ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            logger.error(traceback.format_exc())
        
        try:
            logger_manager = LoggerManager(
                "fetch_daily",
                log_dir=log_dir,
                console_level=log_level,
                file_level=logging.DEBUG,
                is_mock=config.mock_mode
            )
            logger = logger_manager.get_logger()
            logger.info("LoggerManagerã®åˆæœŸåŒ–ã«æˆåŠŸã—ã¾ã—ãŸ")
        except Exception as e:
            logger.error(f"LoggerManagerã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            logger.error(traceback.format_exc())
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ­ã‚¬ãƒ¼ã‚’ä½¿ç”¨
            logger.warning("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ­ã‚¬ãƒ¼ã‚’ä½¿ç”¨ã—ã¾ã™")
        
        # Enable debug mode if configured
        if config.debug_mode:
            try:
                logger_manager.set_debug_mode(True)
                logger.info("ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–ã—ã¾ã—ãŸ")
            except Exception as e:
                logger.error(f"ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®æœ‰åŠ¹åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        
        # Log environment type
        env_type = "Lambda" if os.environ.get('AWS_LAMBDA_EXECUTION', '').lower() == 'true' else "Local"
        if config.mock_mode:
            env_type += " (Mock)"
        logger.info(f"ğŸ”§ Running in {env_type} environment")
        
        # Set up API client
        api_client = AlphaVantageClient(config.api_key, config.api_base_url)
        api_client.set_logger(logger)
        
        # Set up data processor
        data_processor = StockDataProcessor()
        data_processor.set_logger(logger)
        
        # Set up S3 storage
        s3_storage = S3Storage(config.s3_bucket, config.s3_region)
        s3_storage.set_logger(logger)
        
        # Set up atomic S3 updates
        atomic_s3 = AtomicS3(s3_storage)
        atomic_s3.set_logger(logger)
        
        # Set up alert manager
        alert_manager = AlertManager(
            config.email_config,
            config.slack_webhook_url,
            config.slack_webhook_url_error,
            config.slack_webhook_url_warning,
            config.slack_webhook_url_info
        )
        alert_manager.set_logger(logger)
        
        # Slackè¨­å®šã®è©³ç´°ã‚’ãƒ­ã‚°ã«å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã®ã¿ï¼‰
        if config.debug_mode:
            logger.debug("=" * 80)
            logger.debug("Slackè¨­å®šã®è©³ç´°:")
            logger.debug(f"config.slack_enabled: {config.slack_enabled}")
            logger.debug(f"config.slack_webhook_url: {config.slack_webhook_url}")
            logger.debug(f"config.slack_webhook_url_error: {config.slack_webhook_url_error}")
            logger.debug(f"config.slack_webhook_url_warning: {config.slack_webhook_url_warning}")
            logger.debug(f"config.slack_webhook_url_info: {config.slack_webhook_url_info}")
            
            # AlertManagerã®å†…éƒ¨çŠ¶æ…‹ã‚’ç¢ºèª
            try:
                logger.debug("AlertManagerã®å†…éƒ¨çŠ¶æ…‹:")
                if hasattr(alert_manager, 'slack_webhook_url'):
                    logger.debug(f"alert_manager.slack_webhook_url: {alert_manager.slack_webhook_url}")
                if hasattr(alert_manager, 'slack_webhook_url_error'):
                    logger.debug(f"alert_manager.slack_webhook_url_error: {alert_manager.slack_webhook_url_error}")
                if hasattr(alert_manager, 'slack_webhook_url_warning'):
                    logger.debug(f"alert_manager.slack_webhook_url_warning: {alert_manager.slack_webhook_url_warning}")
                if hasattr(alert_manager, 'slack_webhook_url_info'):
                    logger.debug(f"alert_manager.slack_webhook_url_info: {alert_manager.slack_webhook_url_info}")
            except Exception as e:
                logger.error(f"AlertManagerã®å†…éƒ¨çŠ¶æ…‹ç¢ºèªä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®Slacké€šçŸ¥ã‚’é€ä¿¡ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã®ã¿ï¼‰
        if config.debug_mode and config.slack_enabled and not os.environ.get('AWS_LAMBDA_EXECUTION', '').lower() == 'true':
            try:
                logger.debug("ãƒ†ã‚¹ãƒˆç”¨ã®Slacké€šçŸ¥ã‚’é€ä¿¡ã—ã¾ã™...")
                test_message = "ğŸ” This is a test message from fetch_daily.py"
                test_details = "Debug mode is enabled. This is just a test to verify Slack notifications are working."
                
                # ãƒ†ã‚¹ãƒˆç”¨ã®é€šçŸ¥ã‚’é€ä¿¡
                alert_manager.send_info_alert(
                    test_message,
                    test_details,
                    source="fetch_daily.py (DEBUG)",
                    send_email=False,
                    send_slack=True
                )
                logger.debug("âœ… ãƒ†ã‚¹ãƒˆé€šçŸ¥ã®é€ä¿¡ã«æˆåŠŸã—ã¾ã—ãŸ")
            except Exception as e:
                logger.error(f"âŒ ãƒ†ã‚¹ãƒˆé€šçŸ¥ã®é€ä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                logger.error(traceback.format_exc())
        
        if config.debug_mode:
            logger.debug("=" * 80)
        
        return logger, api_client, data_processor, s3_storage, atomic_s3, alert_manager
    except Exception as e:
        logger.error(f"setup_components é–¢æ•°ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logger.error(traceback.format_exc())
        return None, None, None, None, None, None

@log_execution_time(logging.getLogger(__name__))
def process_symbol(symbol, config, api_client, data_processor, atomic_s3, logger):
    """
    Process a single stock symbol.
    
    Args:
        symbol: Stock symbol to process
        config: Configuration object
        api_client: Alpha Vantage API client
        data_processor: Data processor
        atomic_s3: Atomic S3 updater
        logger: Logger
        
    Returns:
        Boolean indicating success
    """
    logger.info(f"ğŸ” Processing symbol: {symbol}")
    
    # Fetch data from API
    stock_data = api_client.fetch_daily_stock_data(symbol)
    
    if not stock_data:
        logger.error(f"âŒ Failed to fetch data for {symbol}")
        return False
    
    # Validate and transform data
    is_valid, df = data_processor.validate_and_transform(stock_data)
    
    if not is_valid or df is None:
        logger.error(f"âŒ Data validation failed for {symbol}")
        return False
    
    # Extract latest data point
    latest_df = data_processor.extract_latest_data(df)
    latest_date = latest_df.index[0].strftime('%Y-%m-%d')
    
    # Convert to JSON
    json_data = data_processor.convert_to_json(df)
    latest_json_data = data_processor.convert_to_json(latest_df)
    
    # Add metadata
    json_data['symbol'] = symbol
    json_data['last_updated'] = datetime.now().isoformat()
    latest_json_data['symbol'] = symbol
    latest_json_data['last_updated'] = datetime.now().isoformat()
    
    # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆï¼ˆä»Šå›ã¯ç”Ÿãƒ‡ãƒ¼ã‚¿ã¨åŒã˜ï¼‰
    # å°†æ¥çš„ã«ã¯ã€ã“ã“ã§è¿½åŠ ã®å‡¦ç†ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™
    processed_json_data = json_data.copy()
    processed_latest_json_data = latest_json_data.copy()
    
    # æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã§ã®S3ã‚­ãƒ¼ã‚’ç”Ÿæˆ
    # ç”Ÿãƒ‡ãƒ¼ã‚¿ç”¨ã®ã‚­ãƒ¼
    raw_full_key = config.get_s3_key_v2(symbol, 'raw')
    raw_latest_key = config.get_s3_key_v2(symbol, 'raw', is_latest=True)
    raw_daily_key = config.get_s3_key_v2(symbol, 'raw', date=latest_date)
    raw_metadata_key = config.get_metadata_key_v2(symbol, 'raw')
    
    # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ç”¨ã®ã‚­ãƒ¼
    processed_full_key = config.get_s3_key_v2(symbol, 'processed')
    processed_latest_key = config.get_s3_key_v2(symbol, 'processed', is_latest=True)
    processed_daily_key = config.get_s3_key_v2(symbol, 'processed', date=latest_date)
    processed_metadata_key = config.get_metadata_key_v2(symbol, 'processed')
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
    metadata = {
        'symbol': symbol,
        'last_updated': datetime.now().isoformat(),
        'latest_date': latest_date,
        'data_points': len(df),
        'date_range': {
            'start': df.index[-1].strftime('%Y-%m-%d'),
            'end': latest_date
        }
    }
    
    # ç”Ÿãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    logger.info(f"ğŸ”„ Saving raw data for {symbol}...")
    
    # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    if not atomic_s3.atomic_json_update(raw_latest_key, latest_json_data):
        logger.error(f"âŒ Failed to save raw latest data for {symbol}")
        return False
    
    # æ—¥åˆ¥ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    if not atomic_s3.atomic_json_update(raw_daily_key, latest_json_data):
        logger.warning(f"âš ï¸ Failed to save raw daily data for {symbol}, but latest data was saved")
    
    # å…¨æœŸé–“ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    if not atomic_s3.atomic_json_update(raw_full_key, json_data):
        logger.warning(f"âš ï¸ Failed to update raw full data for {symbol}, but latest data was saved")
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    if not atomic_s3.atomic_json_update(raw_metadata_key, metadata):
        logger.warning(f"âš ï¸ Failed to update raw metadata for {symbol}")
    
    # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    logger.info(f"ğŸ”„ Saving processed data for {symbol}...")
    
    # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    if not atomic_s3.atomic_json_update(processed_latest_key, processed_latest_json_data):
        logger.warning(f"âš ï¸ Failed to save processed latest data for {symbol}")
    
    # æ—¥åˆ¥ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    if not atomic_s3.atomic_json_update(processed_daily_key, processed_latest_json_data):
        logger.warning(f"âš ï¸ Failed to save processed daily data for {symbol}")
    
    # å…¨æœŸé–“ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    if not atomic_s3.atomic_json_update(processed_full_key, processed_json_data):
        logger.warning(f"âš ï¸ Failed to update processed full data for {symbol}")
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    if not atomic_s3.atomic_json_update(processed_metadata_key, metadata):
        logger.warning(f"âš ï¸ Failed to update processed metadata for {symbol}")
    
    logger.info(f"âœ… Successfully processed {symbol} for date {latest_date}")
    return True

def main():
    """Main function."""
    # æ¨™æº–ãƒ­ã‚¬ãƒ¼ã‚’åˆæœŸåŒ–
    logger = logging.getLogger(__name__)
    
    start_time = time.time()
    
    # Load configuration
    config = Config()
    
    # Lambdaç’°å¢ƒã‹ã©ã†ã‹ã‚’åˆ¤å®š
    is_lambda = os.environ.get('AWS_LAMBDA_EXECUTION', '').lower() == 'true'
    
    if is_lambda:
        # Lambdaç’°å¢ƒã§ã¯ç’°å¢ƒå¤‰æ•°ã®è¨­å®šã‚’å„ªå…ˆ
        mock_mode_env = os.environ.get('MOCK_MODE', 'false').lower()
        debug_mode_env = os.environ.get('DEBUG_MODE', 'false').lower()
        config.mock_mode = mock_mode_env == 'true'
        config.debug_mode = debug_mode_env == 'true'
        config.save_to_s3 = True
    else:
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ãƒ¢ãƒ¼ãƒ‰è¨­å®šã‚’èª­ã¿è¾¼ã‚€
        mock_mode_env = os.environ.get('MOCK_MODE', 'false').lower()
        debug_mode_env = os.environ.get('DEBUG_MODE', 'false').lower()
        save_to_s3_env = os.environ.get('SAVE_TO_S3', 'false').lower()
        
        config.mock_mode = mock_mode_env == 'true'
        config.debug_mode = debug_mode_env == 'true'
        config.save_to_s3 = save_to_s3_env == 'true'
    
    # Set up components
    logger, api_client, data_processor, s3_storage, atomic_s3, alert_manager = setup_components(config)
    
    # env_typeã®å®šç¾©
    env_type = "Lambda" if is_lambda else "Local"
    if config.mock_mode:
        env_type += " (Mock)"
    
    # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã®ã¿Slackæ¥ç¶šãƒ†ã‚¹ãƒˆ
    if config.debug_mode and config.slack_enabled:
        logger.info("Slackæ¥ç¶šãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™...")
        try:
            test_result = alert_manager.test_slack_connection()
            if test_result:
                logger.info("âœ… Slackæ¥ç¶šãƒ†ã‚¹ãƒˆã«æˆåŠŸã—ã¾ã—ãŸ")
            else:
                logger.error("âŒ Slackæ¥ç¶šãƒ†ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        except Exception as e:
            logger.error(f"âŒ Slackæ¥ç¶šãƒ†ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
    # Log execution start
    logger.info("=" * 80)
    logger.info(f"ğŸš€ Starting daily stock data fetch at {datetime.now().isoformat()}")
    logger.info(f"ğŸ“‹ Configuration: {config}")
    logger.info(f"ğŸ” Environment STOCK_SYMBOLS: {os.getenv('STOCK_SYMBOLS')}")
    logger.info(f"ğŸ” Config stock_symbols: {config.stock_symbols}")
    logger.info("=" * 80)
    
    # Process each symbol
    results = {}
    success_count = 0
    failure_count = 0
    
    for symbol in config.stock_symbols:
        try:
            success = process_symbol(symbol, config, api_client, data_processor, atomic_s3, logger)
            results[symbol] = "SUCCESS" if success else "FAILURE"
            
            if success:
                success_count += 1
            else:
                failure_count += 1
                
        except Exception as e:
            logger.exception(f"âŒ Unexpected error processing {symbol}: {e}")
            results[symbol] = f"ERROR: {str(e)}"
            failure_count += 1
    
    # Calculate execution time
    end_time = time.time()
    execution_time = end_time - start_time
    
    # Log execution summary
    logger.info("=" * 80)
    logger.info(f"ğŸ“Š Execution summary:")
    logger.info(f"â± Total execution time: {execution_time:.2f} seconds")
    logger.info(f"âœ… Successful: {success_count}")
    logger.info(f"âŒ Failed: {failure_count}")
    logger.info(f"ğŸ“‹ Results by symbol: {json.dumps(results, indent=2)}")
    logger.info("=" * 80)
    
    # å®Ÿè¡Œçµæœã®é€šçŸ¥
    if config.slack_enabled:
        try:
            # å®Ÿè¡Œç’°å¢ƒæƒ…å ±ã‚’å–å¾—
            env_info = f"Environment: {env_type}"
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            # å…±é€šã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
            common_fields = [
                {"title": "Environment", "value": env_type, "short": True},
                {"title": "Execution Time", "value": f"{execution_time:.2f} seconds", "short": True},
                {"title": "Timestamp", "value": timestamp, "short": True},
            ]
            
            # çµæœã«åŸºã¥ã„ã¦é€šçŸ¥ã‚’é€ä¿¡
            if failure_count > 0:
                # å¤±æ•—ã—ãŸã‚·ãƒ³ãƒœãƒ«ã‚’æŠ½å‡º
                failed_symbols = [symbol for symbol, result in results.items() if result != "SUCCESS"]
                
                # å¤±æ•—æƒ…å ±ã‚’è©³ç´°ã«å«ã‚ã‚‹
                failure_fields = [
                    {"title": "Failed Symbols", "value": ", ".join(failed_symbols), "short": False},
                    {"title": "Success Count", "value": str(success_count), "short": True},
                    {"title": "Failure Count", "value": str(failure_count), "short": True}
                ]
                
                # è©³ç´°ãªçµæœæƒ…å ±
                detailed_results = "\n".join([f"{symbol}: {result}" for symbol, result in results.items()])
                
                # è­¦å‘Šã‚¢ãƒ©ãƒ¼ãƒˆã‚’é€ä¿¡
                alert_message = f"âš ï¸ Daily stock data fetch completed with {failure_count} failures"
                alert_details = f"""
WARNING: Some stock data fetch operations failed.

Execution time: {execution_time:.2f} seconds
Environment: {env_type}
Successful: {success_count}
Failed: {failure_count}

Failed symbols: {', '.join(failed_symbols)}

Detailed results:
{detailed_results}
"""
                alert_manager.send_warning_alert(
                    alert_message,
                    alert_details,
                    source="fetch_daily.py",
                    send_email=config.email_enabled,
                    send_slack=True,
                    additional_fields=common_fields + failure_fields
                )
                logger.info("âœ… è­¦å‘Šé€šçŸ¥ã‚’é€ä¿¡ã—ã¾ã—ãŸ")
            else:
                # æˆåŠŸã—ãŸã‚·ãƒ³ãƒœãƒ«ã‚’æŠ½å‡º
                successful_symbols = [symbol for symbol, result in results.items() if result == "SUCCESS"]
                
                # æˆåŠŸæƒ…å ±ã‚’è©³ç´°ã«å«ã‚ã‚‹
                success_fields = [
                    {"title": "Successful Symbols", "value": ", ".join(successful_symbols), "short": False},
                    {"title": "Total Successful", "value": str(success_count), "short": True}
                ]
                
                # æˆåŠŸã‚¢ãƒ©ãƒ¼ãƒˆã‚’é€ä¿¡
                alert_message = f"âœ… Daily stock data fetch completed successfully for all {success_count} symbols"
                alert_details = f"""
INFO: Stock data fetch summary.

Execution time: {execution_time:.2f} seconds
Environment: {env_type}
Successful symbols: {', '.join(successful_symbols)}
Total successful: {success_count}
"""
                alert_manager.send_success_alert(
                    alert_message,
                    alert_details,
                    source="fetch_daily.py",
                    send_email=config.email_enabled,
                    send_slack=True,
                    additional_fields=common_fields + success_fields
                )
                logger.info("âœ… æˆåŠŸé€šçŸ¥ã‚’é€ä¿¡ã—ã¾ã—ãŸ")
        except Exception as e:
            logger.error(f"âŒ Slacké€šçŸ¥å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    else:
        logger.info("Slacké€šçŸ¥ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
    
    # Return exit code
    return 0 if failure_count == 0 else 1

if __name__ == "__main__":
    try:
        sys.exit(main())
    except Exception as e:
        # Catch any unexpected exceptions
        print(f"âŒ Critical error: {e}")
        traceback.print_exc()
        sys.exit(1)
