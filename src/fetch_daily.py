#!/usr/bin/env python3
"""
Daily Stock Data Fetcher

This script fetches the latest daily stock data from Alpha Vantage API
and stores it in AWS S3. It is designed to be run as a daily batch job.
"""

import os
import sys
import time
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
import traceback

# ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    from src.config import Config
    from src.utils.api_client import AlphaVantageClient
    from src.utils.data_processing import StockDataProcessor
    from src.utils.storage import S3Storage
    from src.utils.atomic_s3 import AtomicS3
    from src.utils.logging_utils import LoggerManager, log_execution_time
    from src.utils.alerts import AlertManager
    from src.utils.local_storage import LocalStorage
except ImportError:
    # ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    from config import Config
    from utils.api_client import AlphaVantageClient
    from utils.data_processing import StockDataProcessor
    from utils.storage import S3Storage
    from utils.atomic_s3 import AtomicS3
    from utils.logging_utils import LoggerManager, log_execution_time
    from utils.alerts import AlertManager
    from utils.local_storage import LocalStorage

def setup_components(config):
    """
    Set up all components with the given configuration.
    
    Args:
        config: Configuration object
        
    Returns:
        Tuple of (logger, api_client, data_processor, s3_storage, atomic_s3, alert_manager, local_storage)
    """
    # ãƒ­ã‚¬ãƒ¼åˆæœŸåŒ–
    logger = logging.getLogger(__name__)
    
    try:
        # Set up logger
        log_level = getattr(logging, config.log_level.upper(), logging.INFO)
        
        # Lambdaç’°å¢ƒã®å ´åˆã¯ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’/tmpä»¥ä¸‹ã«è¨­å®š
        if os.environ.get('AWS_LAMBDA_EXECUTION', '').lower() == 'true':
            log_dir = Path('/tmp/logs')
            logger.info(f"Lambdaç’°å¢ƒã®ãŸã‚ã€ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ {log_dir} ã«è¨­å®šã—ã¾ã™")
        else:
            log_dir = config.log_dir
            logger.info(f"ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®ãŸã‚ã€ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ {log_dir} ã«è¨­å®šã—ã¾ã™")
            
        # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        try:
            os.makedirs(log_dir, exist_ok=True)
            logger.info(f"ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ: {log_dir}")
        except Exception as e:
            logger.error(f"ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            logger.error(traceback.format_exc())
        
        try:
            logger_manager = LoggerManager(
                "fetch_daily",
                log_dir=log_dir,
                console_level=log_level,
                file_level=logging.DEBUG,
                is_mock=config.mock_mode
            )
            logger = logger_manager.get_logger()
            logger.info("LoggerManagerã®åˆæœŸåŒ–ã«æˆåŠŸã—ã¾ã—ãŸ")
        except Exception as e:
            logger.error(f"LoggerManagerã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            logger.error(traceback.format_exc())
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ­ã‚¬ãƒ¼ã‚’ä½¿ç”¨
            logger.warning("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ­ã‚¬ãƒ¼ã‚’ä½¿ç”¨ã—ã¾ã™")
        
        # Enable debug mode if configured
        if config.debug_mode:
            try:
                logger_manager.set_debug_mode(True)
                logger.info("ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–ã—ã¾ã—ãŸ")
            except Exception as e:
                logger.error(f"ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®æœ‰åŠ¹åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        
        # Log environment type
        env_type = "Lambda" if os.environ.get('AWS_LAMBDA_EXECUTION', '').lower() == 'true' else "Local"
        if config.mock_mode:
            env_type += " (Mock)"
        logger.info(f"ğŸ”§ Running in {env_type} environment")
        
        # Set up API client
        api_client = AlphaVantageClient(config.api_key, config.api_base_url)
        api_client.set_logger(logger)
        
        # Set up data processor
        data_processor = StockDataProcessor()
        data_processor.set_logger(logger)
        
        # Set up S3 storage
        s3_storage = S3Storage(config.s3_bucket, config.s3_region)
        s3_storage.set_logger(logger)
        
        # Set up atomic S3 updates
        atomic_s3 = AtomicS3(s3_storage)
        atomic_s3.set_logger(logger)
        
        # Set up alert manager
        alert_manager = AlertManager(
            config.email_config,
            config.slack_webhook_url_error,
            config.slack_webhook_url_warning,
            config.slack_webhook_url_info
        )
        alert_manager.set_logger(logger)
        
        # Slackè¨­å®šã®è©³ç´°ã‚’ãƒ­ã‚°ã«å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã®ã¿ï¼‰
        if config.debug_mode:
            logger.debug("=" * 80)
            logger.debug("Slackè¨­å®šã®è©³ç´°:")
            logger.debug(f"config.slack_enabled: {config.slack_enabled}")
            logger.debug(f"config.slack_webhook_url_error: {config.slack_webhook_url_error}")
            logger.debug(f"config.slack_webhook_url_warning: {config.slack_webhook_url_warning}")
            logger.debug(f"config.slack_webhook_url_info: {config.slack_webhook_url_info}")
            
            # AlertManagerã®å†…éƒ¨çŠ¶æ…‹ã‚’ç¢ºèª
            try:
                logger.debug("AlertManagerã®å†…éƒ¨çŠ¶æ…‹:")
                if hasattr(alert_manager, 'slack_webhook_url_error'):
                    logger.debug(f"alert_manager.slack_webhook_url_error: {alert_manager.slack_webhook_url_error}")
                if hasattr(alert_manager, 'slack_webhook_url_warning'):
                    logger.debug(f"alert_manager.slack_webhook_url_warning: {alert_manager.slack_webhook_url_warning}")
                if hasattr(alert_manager, 'slack_webhook_url_info'):
                    logger.debug(f"alert_manager.slack_webhook_url_info: {alert_manager.slack_webhook_url_info}")
            except Exception as e:
                logger.error(f"AlertManagerã®å†…éƒ¨çŠ¶æ…‹ç¢ºèªä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®Slacké€šçŸ¥ã‚’é€ä¿¡ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã®ã¿ã€ã‹ã¤ãƒ­ãƒ¼ã‚«ãƒ«ã§ãªã„å ´åˆï¼‰
        if config.debug_mode and config.slack_enabled and not os.environ.get('AWS_LAMBDA_EXECUTION', '').lower() == 'true' and config.save_to_s3:
            try:
                logger.debug("ãƒ†ã‚¹ãƒˆç”¨ã®Slacké€šçŸ¥ã‚’é€ä¿¡ã—ã¾ã™...")
                test_message = "ğŸ” This is a test message from fetch_daily.py"
                test_details = "Debug mode is enabled. This is just a test to verify Slack notifications are working."
                
                # ãƒ†ã‚¹ãƒˆç”¨ã®é€šçŸ¥ã‚’é€ä¿¡
                alert_manager.send_info_alert(
                    test_message,
                    test_details,
                    source="fetch_daily.py (DEBUG)",
                    send_email=False,
                    send_slack=True
                )
                logger.debug("âœ… ãƒ†ã‚¹ãƒˆé€šçŸ¥ã®é€ä¿¡ã«æˆåŠŸã—ã¾ã—ãŸ")
            except Exception as e:
                logger.error(f"âŒ ãƒ†ã‚¹ãƒˆé€šçŸ¥ã®é€ä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                logger.error(traceback.format_exc())
        else:
            logger.debug("ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã¾ãŸã¯SAVE_TO_S3=falseã®ãŸã‚ã€ãƒ†ã‚¹ãƒˆé€šçŸ¥ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        
        # Set up local storage
        local_storage = None
        if not config.save_to_s3:
            try:
                local_storage = LocalStorage(config.local_storage_dir)
                local_storage.set_logger(logger)
                logger.info(f"ğŸ”§ LocalStorage initialized with directory: {config.local_storage_dir}")
            except Exception as e:
                logger.error(f"âŒ LocalStorage initialization failed: {e}")
                logger.error(traceback.format_exc())
        
        if config.debug_mode:
            logger.debug("=" * 80)
        
        return logger, api_client, data_processor, s3_storage, atomic_s3, alert_manager, local_storage
    except Exception as e:
        logger.error(f"setup_components é–¢æ•°ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logger.error(traceback.format_exc())
        return None, None, None, None, None, None, None

@log_execution_time(logging.getLogger(__name__))
def process_symbol(symbol, config, api_client, data_processor, atomic_s3, logger, local_storage=None):
    """
    Process a single stock symbol.
    
    Args:
        symbol: Stock symbol to process
        config: Configuration object
        api_client: Alpha Vantage API client
        data_processor: Data processor
        atomic_s3: Atomic S3 updater
        logger: Logger
        local_storage: Local storage (optional)
        
    Returns:
        Tuple of (Boolean indicating success, latest_date)
    """
    logger.info(f"ğŸ” Processing symbol: {symbol}")
    
    # Fetch data from API
    stock_data = api_client.fetch_daily_stock_data(symbol)
    
    if not stock_data:
        logger.error(f"âŒ Failed to fetch data for {symbol}")
        return False
    
    # Validate and transform data
    is_valid, df = data_processor.validate_and_transform(stock_data)
    
    if not is_valid or df is None:
        logger.error(f"âŒ Data validation failed for {symbol}")
        return False
    
    # Extract latest data point
    latest_df = data_processor.extract_latest_data(df)
    api_date = latest_df.index[0].strftime('%Y-%m-%d')
    
    # å‰æ—¥ã®æ—¥ä»˜ã‚’è¨ˆç®—ï¼ˆJSTåŸºæº–ï¼‰
    now = datetime.now()
    yesterday = (now - timedelta(days=1)).strftime('%Y-%m-%d')
    
    # APIã‹ã‚‰å–å¾—ã—ãŸæ—¥ä»˜ã‚’ä½¿ç”¨ï¼ˆJSONã®æ—¥ä»˜ã¨ãƒ•ã‚©ãƒ«ãƒ€ã®æ—¥ä»˜ã‚’ä¸€è‡´ã•ã›ã‚‹ï¼‰
    logger.info(f"APIã‹ã‚‰å–å¾—ã—ãŸæ—¥ä»˜: {api_date}, è¨ˆç®—ã•ã‚ŒãŸå‰æ—¥ã®æ—¥ä»˜: {yesterday}")
    if api_date < yesterday:
        logger.warning(f"âš ï¸ APIã‹ã‚‰å–å¾—ã—ãŸæ—¥ä»˜({api_date})ãŒå‰æ—¥({yesterday})ã‚ˆã‚Šå¤ã„ã§ã™ã€‚ã“ã‚Œã¯åœŸæ—¥ã‚„ç¥æ—¥ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
    
    # JSONã®æ—¥ä»˜ã¨ãƒ•ã‚©ãƒ«ãƒ€ã®æ—¥ä»˜ã‚’ä¸€è‡´ã•ã›ã‚‹ãŸã‚ã€APIã‹ã‚‰å–å¾—ã—ãŸæ—¥ä»˜ã‚’ä½¿ç”¨
    latest_date = api_date
    
    # Convert to JSON
    json_data = data_processor.convert_to_json(df)
    latest_json_data = data_processor.convert_to_json(latest_df)
    
    # Add metadata
    json_data['symbol'] = symbol
    json_data['last_updated'] = datetime.now().isoformat()
    latest_json_data['symbol'] = symbol
    latest_json_data['last_updated'] = datetime.now().isoformat()
    
    # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆï¼ˆä»Šå›ã¯ç”Ÿãƒ‡ãƒ¼ã‚¿ã¨åŒã˜ï¼‰
    # å°†æ¥çš„ã«ã¯ã€ã“ã“ã§è¿½åŠ ã®å‡¦ç†ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™
    processed_json_data = json_data.copy()
    processed_latest_json_data = latest_json_data.copy()
    
    # æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã§ã®S3ã‚­ãƒ¼ã‚’ç”Ÿæˆ
    # ç”Ÿãƒ‡ãƒ¼ã‚¿ç”¨ã®ã‚­ãƒ¼
    raw_full_key = config.get_s3_key_v2(symbol, 'raw')
    raw_latest_key = config.get_s3_key_v2(symbol, 'raw', is_latest=True)
    raw_daily_key = config.get_s3_key_v2(symbol, 'raw', date=latest_date)
    raw_metadata_key = config.get_metadata_key_v2(symbol, 'raw')
    
    # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ç”¨ã®ã‚­ãƒ¼
    processed_full_key = config.get_s3_key_v2(symbol, 'processed')
    processed_latest_key = config.get_s3_key_v2(symbol, 'processed', is_latest=True)
    processed_daily_key = config.get_s3_key_v2(symbol, 'processed', date=latest_date)
    processed_metadata_key = config.get_metadata_key_v2(symbol, 'processed')
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
    metadata = {
        'symbol': symbol,
        'last_updated': datetime.now().isoformat(),
        'latest_date': latest_date,
        'data_points': len(df),
        'date_range': {
            'start': df.index[-1].strftime('%Y-%m-%d'),
            'end': latest_date
        }
    }
    
    # ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    if config.save_to_s3:
        # S3ã«ä¿å­˜
        logger.info(f"ğŸ”„ Saving data to S3 for {symbol}...")
        
        # ç”Ÿãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        if not atomic_s3.atomic_json_update(raw_latest_key, latest_json_data):
            logger.error(f"âŒ Failed to save raw latest data for {symbol}")
            return False
        
        # æ—¥åˆ¥ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        if not atomic_s3.atomic_json_update(raw_daily_key, latest_json_data):
            logger.warning(f"âš ï¸ Failed to save raw daily data for {symbol}, but latest data was saved")
        
        # å…¨æœŸé–“ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        if not atomic_s3.atomic_json_update(raw_full_key, json_data):
            logger.warning(f"âš ï¸ Failed to update raw full data for {symbol}, but latest data was saved")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        if not atomic_s3.atomic_json_update(raw_metadata_key, metadata):
            logger.warning(f"âš ï¸ Failed to update raw metadata for {symbol}")
        
        # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        logger.info(f"ğŸ”„ Saving processed data to S3 for {symbol}...")
        
        # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        if not atomic_s3.atomic_json_update(processed_latest_key, processed_latest_json_data):
            logger.warning(f"âš ï¸ Failed to save processed latest data for {symbol}")
        
        # æ—¥åˆ¥ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        if not atomic_s3.atomic_json_update(processed_daily_key, processed_latest_json_data):
            logger.warning(f"âš ï¸ Failed to save processed daily data for {symbol}")
        
        # å…¨æœŸé–“ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        if not atomic_s3.atomic_json_update(processed_full_key, processed_json_data):
            logger.warning(f"âš ï¸ Failed to update processed full data for {symbol}")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        if not atomic_s3.atomic_json_update(processed_metadata_key, metadata):
            logger.warning(f"âš ï¸ Failed to update processed metadata for {symbol}")
    elif local_storage:
        # ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜
        logger.info(f"ğŸ”„ Saving data to local storage for {symbol}...")
        
        # Lambdaé–¢æ•°ã¨åŒã˜æ§‹é€ ã§ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜
        # 1. æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿
        local_daily_key = f"daily/{symbol}/{latest_date}.json"
        if not local_storage.put_object(local_daily_key, latest_json_data):
            logger.warning(f"âš ï¸ Failed to save daily data to local storage for {symbol}")
        else:
            logger.info(f"âœ… Saved daily data to local storage: {local_daily_key}")
        
        # 2. æœ€æ–°ãƒ‡ãƒ¼ã‚¿
        local_latest_key = f"{symbol}/latest.json"
        if not local_storage.put_object(local_latest_key, latest_json_data):
            logger.warning(f"âš ï¸ Failed to save latest data to local storage for {symbol}")
        else:
            logger.info(f"âœ… Saved latest data to local storage: {local_latest_key}")
        
        # 3. å…¨æœŸé–“ãƒ‡ãƒ¼ã‚¿
        local_full_key = f"{symbol}/full.json"
        if not local_storage.put_object(local_full_key, json_data):
            logger.warning(f"âš ï¸ Failed to save full data to local storage for {symbol}")
        else:
            logger.info(f"âœ… Saved full data to local storage: {local_full_key}")
        
        # 4. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        local_metadata_key = f"{symbol}/metadata.json"
        if not local_storage.put_object(local_metadata_key, metadata):
            logger.warning(f"âš ï¸ Failed to save metadata to local storage for {symbol}")
        else:
            logger.info(f"âœ… Saved metadata to local storage: {local_metadata_key}")
    else:
        logger.warning(f"âš ï¸ No storage method available for {symbol}. Data will not be saved.")
        return False
    
    logger.info(f"âœ… Successfully processed {symbol} for date {latest_date}")
    return True, latest_date

def main():
    """Main function."""
    # æ¨™æº–ãƒ­ã‚¬ãƒ¼ã‚’åˆæœŸåŒ–
    logger = logging.getLogger(__name__)
    
    # ç’°å¢ƒå¤‰æ•°ã®è¨­å®šã‚’ãƒ­ã‚°ã«å‡ºåŠ›ï¼ˆæ¨™æº–ãƒ­ã‚¬ãƒ¼ã‚’ä½¿ç”¨ï¼‰
    print(f"Environment variables:")
    print(f"MOCK_MODE: {os.environ.get('MOCK_MODE', 'not set')}")
    print(f"DEBUG_MODE: {os.environ.get('DEBUG_MODE', 'not set')}")
    print(f"SAVE_TO_S3: {os.environ.get('SAVE_TO_S3', 'not set')}")
    
    start_time = time.time()
    
    # Load configuration
    config = Config()
    
    # Lambdaç’°å¢ƒã‹ã©ã†ã‹ã‚’åˆ¤å®š
    is_lambda = os.environ.get('AWS_LAMBDA_EXECUTION', '').lower() == 'true'
    
    if is_lambda:
        # Lambdaç’°å¢ƒã§ã¯ç’°å¢ƒå¤‰æ•°ã®è¨­å®šã‚’å„ªå…ˆ
        mock_mode_env = os.environ.get('MOCK_MODE', 'false').lower()
        debug_mode_env = os.environ.get('DEBUG_MODE', 'false').lower()
        config.mock_mode = mock_mode_env == 'true'
        config.debug_mode = debug_mode_env == 'true'
        config.save_to_s3 = True
    else:
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ãƒ¢ãƒ¼ãƒ‰è¨­å®šã‚’èª­ã¿è¾¼ã‚€
        # MOCK_MODE: ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ (true/false)
        # - true: APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã›ãšã€ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        # - false: å®Ÿéš›ã«APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        mock_mode_env = os.environ.get('MOCK_MODE', 'false').lower()
        
        # DEBUG_MODE: ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ã©ã†ã‹ (true/false)
        # - true: è©³ç´°ãªãƒ­ã‚°å‡ºåŠ›ã€ãƒ†ã‚¹ãƒˆé€šçŸ¥ã®é€ä¿¡ãªã©
        # - false: é€šå¸¸ã®å‹•ä½œ
        debug_mode_env = os.environ.get('DEBUG_MODE', 'false').lower()
        
        # SAVE_TO_S3: S3ã«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹ (true/false)
        # - true: S3ã«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        # - false: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        save_to_s3_env = os.environ.get('SAVE_TO_S3', 'false').lower()
        
        config.mock_mode = mock_mode_env == 'true'
        config.debug_mode = debug_mode_env == 'true'
        config.save_to_s3 = save_to_s3_env == 'true'
        
        # ç’°å¢ƒå¤‰æ•°ã®è¨­å®šã‚’ãƒ­ã‚°ã«å‡ºåŠ›
        logger.debug(f"Environment variables:")
        logger.debug(f"MOCK_MODE: {os.environ.get('MOCK_MODE', 'not set')}")
        logger.debug(f"DEBUG_MODE: {os.environ.get('DEBUG_MODE', 'not set')}")
        logger.debug(f"SAVE_TO_S3: {os.environ.get('SAVE_TO_S3', 'not set')}")
        logger.debug(f"Parsed values:")
        logger.debug(f"mock_mode_env: {mock_mode_env}")
        logger.debug(f"debug_mode_env: {debug_mode_env}")
        logger.debug(f"save_to_s3_env: {save_to_s3_env}")
    
    # Set up components
    logger, api_client, data_processor, s3_storage, atomic_s3, alert_manager, local_storage = setup_components(config)
    
    # env_typeã®å®šç¾©
    env_type = "Lambda" if is_lambda else "Local"
    if config.mock_mode:
        env_type += " (Mock)"
    
    # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã®ã¿Slackæ¥ç¶šãƒ†ã‚¹ãƒˆ
    if config.debug_mode and config.slack_enabled:
        logger.info("Slackæ¥ç¶šãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™...")
        try:
            # infoç”¨ã®Webhook URLã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚¹ãƒˆ
            test_result = alert_manager.test_slack_connection(webhook_url=config.slack_webhook_url_info)
            if test_result:
                logger.info("âœ… Slackæ¥ç¶šãƒ†ã‚¹ãƒˆã«æˆåŠŸã—ã¾ã—ãŸ")
            else:
                logger.error("âŒ Slackæ¥ç¶šãƒ†ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        except Exception as e:
            logger.error(f"âŒ Slackæ¥ç¶šãƒ†ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
    # Log execution start
    logger.info("=" * 80)
    logger.info(f"ğŸš€ Starting daily stock data fetch at {datetime.now().isoformat()}")
    logger.info(f"ğŸ“‹ Configuration: {config}")
    logger.info(f"ğŸ” Environment STOCK_SYMBOLS: {os.getenv('STOCK_SYMBOLS')}")
    logger.info(f"ğŸ” Config stock_symbols: {config.stock_symbols}")
    logger.info(f"ğŸ” SAVE_TO_S3: {config.save_to_s3}")
    logger.info(f"ğŸ” LOCAL_STORAGE: {'Initialized' if local_storage else 'Not initialized'}")
    logger.info("=" * 80)
    
    # Process each symbol
    results = {}
    success_count = 0
    failure_count = 0
    latest_date = None  # æœ€æ–°ã®æ—¥ä»˜ã‚’ä¿æŒã™ã‚‹å¤‰æ•°
    symbol_dates = {}   # å„ã‚·ãƒ³ãƒœãƒ«ã®æ—¥ä»˜ã‚’ä¿æŒã™ã‚‹è¾æ›¸
    
    for symbol in config.stock_symbols:
        try:
            result = process_symbol(symbol, config, api_client, data_processor, atomic_s3, logger, local_storage)
            
            if isinstance(result, tuple) and len(result) == 2:
                success, symbol_date = result
                # å„ã‚·ãƒ³ãƒœãƒ«ã®æ—¥ä»˜ã‚’ä¿å­˜
                if success and symbol_date:
                    symbol_dates[symbol] = symbol_date
                    # æœ€æ–°ã®æ—¥ä»˜ã‚’æ›´æ–°ï¼ˆåˆå›ã®ã¿ï¼‰
                    if not latest_date:
                        latest_date = symbol_date
            else:
                success = result
                symbol_date = None
                
            results[symbol] = "SUCCESS" if success else "FAILURE"
            
            if success:
                success_count += 1
            else:
                failure_count += 1
                
        except Exception as e:
            logger.exception(f"âŒ Unexpected error processing {symbol}: {e}")
            results[symbol] = f"ERROR: {str(e)}"
            failure_count += 1
    
    # æœ€æ–°ã®æ—¥ä»˜ãŒå–å¾—ã§ããªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if not latest_date:
        latest_date = datetime.now().strftime('%Y-%m-%d')
        logger.warning(f"æœ€æ–°ã®æ—¥ä»˜ãŒå–å¾—ã§ããªã‹ã£ãŸãŸã‚ã€ç¾åœ¨ã®æ—¥ä»˜ã‚’ä½¿ç”¨ã—ã¾ã™: {latest_date}")
    
    # Calculate execution time
    end_time = time.time()
    execution_time = end_time - start_time
    
    # Log execution summary
    logger.info("=" * 80)
    logger.info(f"ğŸ“Š Execution summary:")
    logger.info(f"â± Total execution time: {execution_time:.2f} seconds")
    logger.info(f"âœ… Successful: {success_count}")
    logger.info(f"âŒ Failed: {failure_count}")
    logger.info(f"ğŸ“‹ Results by symbol: {json.dumps(results, indent=2)}")
    logger.info("=" * 80)
    
    # å®Ÿè¡Œçµæœã®é€šçŸ¥
    if config.slack_enabled:
        try:
            # å®Ÿè¡Œç’°å¢ƒæƒ…å ±ã‚’å–å¾—
            env_info = f"Environment: {env_type}"
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            # UTCã¨JSTã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—
            now_utc = datetime.utcnow()
            now_jst = datetime.now()
            timestamp_utc = now_utc.strftime("%Y-%m-%d %H:%M:%S")
            timestamp_jst = now_jst.strftime("%Y-%m-%d %H:%M:%S")
            
            # å…±é€šã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
            common_fields = [
                {"title": "Environment", "value": env_type, "short": True},
                {"title": "Execution Time", "value": f"{execution_time:.2f} seconds", "short": True},
                {"title": "Timestamp (UTC)", "value": timestamp_utc, "short": True},
                {"title": "Timestamp (JST)", "value": timestamp_jst, "short": True},
            ]
            
            # çµæœã«åŸºã¥ã„ã¦é€šçŸ¥ã‚’é€ä¿¡
            if failure_count > 0:
                # å¤±æ•—ã—ãŸã‚·ãƒ³ãƒœãƒ«ã‚’æŠ½å‡º
                failed_symbols = [symbol for symbol, result in results.items() if result != "SUCCESS"]
                
                # å¤±æ•—æƒ…å ±ã‚’è©³ç´°ã«å«ã‚ã‚‹
                failure_fields = [
                    {"title": "Failed Symbols", "value": ", ".join(failed_symbols), "short": False},
                    {"title": "Success Count", "value": str(success_count), "short": True},
                    {"title": "Failure Count", "value": str(failure_count), "short": True}
                ]
                
                # è©³ç´°ãªçµæœæƒ…å ±
                detailed_results = "\n".join([f"{symbol}: {result}" for symbol, result in results.items()])
                
                # è­¦å‘Šã‚¢ãƒ©ãƒ¼ãƒˆã‚’é€ä¿¡
                alert_message = f"âš ï¸ Daily stock data fetch completed with {failure_count} failures"
                alert_details = f"""
WARNING: Some stock data fetch operations failed.

Execution time: {execution_time:.2f} seconds
Environment: {env_type}
Successful: {success_count}
Failed: {failure_count}

Failed symbols: {', '.join(failed_symbols)}

Detailed results:
{detailed_results}
"""
                # ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã™ã‚‹å ´åˆã¯#local_testãƒãƒ£ãƒ³ãƒãƒ«ã«é€ä¿¡
                if not config.save_to_s3 and config.slack_webhook_url_local_test:
                    alert_manager.send_warning_alert(
                        alert_message,
                        alert_details,
                        source="fetch_daily.py",
                        send_email=config.email_enabled,
                        send_slack=True,
                        additional_fields=common_fields + failure_fields,
                        webhook_url=config.slack_webhook_url_local_test
                    )
                else:
                    alert_manager.send_warning_alert(
                        alert_message,
                        alert_details,
                        source="fetch_daily.py",
                        send_email=config.email_enabled,
                        send_slack=True,
                        additional_fields=common_fields + failure_fields
                    )
                logger.info("âœ… è­¦å‘Šé€šçŸ¥ã‚’é€ä¿¡ã—ã¾ã—ãŸ")
            else:
                # æˆåŠŸã—ãŸã‚·ãƒ³ãƒœãƒ«ã‚’æŠ½å‡º
                successful_symbols = [symbol for symbol, result in results.items() if result == "SUCCESS"]
                
                # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã®è¨ˆç®—ã¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹æƒ…å ±ã®åé›†ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã®å ´åˆï¼‰
                data_size = 0
                data_sizes = {}
                local_file_paths = []
                
                if not config.save_to_s3 and local_storage:
                    try:
                        for symbol in successful_symbols:
                            # æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºã¨ãƒ‘ã‚¹
                            daily_path = os.path.join(config.local_storage_dir, f"daily/{symbol}/{latest_date}.json")
                            if os.path.exists(daily_path):
                                daily_size = os.path.getsize(daily_path) / 1024  # KB
                                if 'daily' not in data_sizes:
                                    data_sizes['daily'] = 0
                                data_sizes['daily'] += daily_size
                                data_size += daily_size
                                local_file_paths.append(f"- æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿: {daily_path}")
                            
                            # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºã¨ãƒ‘ã‚¹
                            latest_path = os.path.join(config.local_storage_dir, f"{symbol}/latest.json")
                            if os.path.exists(latest_path):
                                latest_size = os.path.getsize(latest_path) / 1024  # KB
                                if 'latest' not in data_sizes:
                                    data_sizes['latest'] = 0
                                data_sizes['latest'] += latest_size
                                data_size += latest_size
                                local_file_paths.append(f"- æœ€æ–°ãƒ‡ãƒ¼ã‚¿: {latest_path}")
                            
                            # å…¨æœŸé–“ãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºã¨ãƒ‘ã‚¹
                            full_path = os.path.join(config.local_storage_dir, f"{symbol}/full.json")
                            if os.path.exists(full_path):
                                full_size = os.path.getsize(full_path) / 1024  # KB
                                if 'full' not in data_sizes:
                                    data_sizes['full'] = 0
                                data_sizes['full'] += full_size
                                data_size += full_size
                                local_file_paths.append(f"- å…¨æœŸé–“ãƒ‡ãƒ¼ã‚¿: {full_path}")
                            
                            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºã¨ãƒ‘ã‚¹
                            metadata_path = os.path.join(config.local_storage_dir, f"{symbol}/metadata.json")
                            if os.path.exists(metadata_path):
                                metadata_size = os.path.getsize(metadata_path) / 1024  # KB
                                if 'metadata' not in data_sizes:
                                    data_sizes['metadata'] = 0
                                data_sizes['metadata'] += metadata_size
                                data_size += metadata_size
                                local_file_paths.append(f"- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {metadata_path}")
                    except Exception as e:
                        logger.warning(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã®è¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                
                # æˆåŠŸæƒ…å ±ã‚’è©³ç´°ã«å«ã‚ã‚‹
                success_fields = [
                    {"title": "Successful Symbols", "value": ", ".join(successful_symbols), "short": False},
                    {"title": "Total Successful", "value": str(success_count), "short": True},
                    {"title": "Data Date", "value": latest_date, "short": True}
                ]
                
                # ãƒ­ãƒ¼ã‚«ãƒ«ã®å ´åˆã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹æƒ…å ±ã‚’è¿½åŠ 
                if not config.save_to_s3 and local_storage and local_file_paths:
                    local_paths_info = "\n".join(local_file_paths)
                    success_fields.append({"title": "Local Storage Paths", "value": local_paths_info, "short": False})
                
                # S3ã®å ´åˆã¯S3ãƒ‘ã‚¹æƒ…å ±ã‚’è¿½åŠ 
                if config.save_to_s3:
                    s3_paths_info = f"""å„ã‚·ãƒ³ãƒœãƒ«ã«ã¤ãä»¥ä¸‹ã®4ç¨®é¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã—ã¾ã—ãŸï¼š
- æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿: .../daily/{latest_date.split('-')[0]}/{latest_date.split('-')[1]}/{latest_date.split('-')[2]}.json
- æœ€æ–°ãƒ‡ãƒ¼ã‚¿: .../latest.json
- å…¨æœŸé–“ãƒ‡ãƒ¼ã‚¿: .../full.json
- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: .../metadata.json"""
                    
                    # S3ãƒ‘ã‚¹ä¾‹ã‚’è¿½åŠ 
                    for symbol in successful_symbols:
                        s3_path_example = f"{symbol}: s3://{config.s3_bucket}/{config.s3_prefix}/stock/raw/{symbol}/daily/{latest_date.split('-')[0]}/{latest_date.split('-')[1]}/{latest_date.split('-')[2]}.json"
                        s3_paths_info += f"\n\n{s3_path_example}"
                    
                    success_fields.append({"title": "S3 Storage Paths", "value": s3_paths_info, "short": False})
                
                # UTCã¨JSTã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—
                now_utc = datetime.utcnow()
                now_jst = datetime.now()
                timestamp_utc = now_utc.strftime("%Y-%m-%d %H:%M:%S")
                timestamp_jst = now_jst.strftime("%Y-%m-%d %H:%M:%S")
                
                # ç’°å¢ƒæƒ…å ±ã‚’å«ã‚€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                env_prefix = "Lambda: " if is_lambda else "Local: "
                alert_message = f"âœ… {env_prefix}Daily stock data fetch completed successfully for all {success_count} symbols"
                
                # è©³ç´°æƒ…å ±
                alert_details = f"""
INFO: Stock data fetch summary.

Execution time: {execution_time:.2f} seconds
Environment: {env_type}
Data date: {latest_date}
Successful symbols: {', '.join(successful_symbols)}
Total successful: {success_count}
"""
                
                # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºæƒ…å ±ã‚’è¿½åŠ ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã®å ´åˆï¼‰
                if not config.save_to_s3 and data_size > 0:
                    alert_details += f"Total data size: {data_size:.2f} KB\n"
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã‚µã‚¤ã‚ºæƒ…å ±
                    if data_sizes:
                        alert_details += "\nãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã‚µã‚¤ã‚º:\n"
                        for file_type, size in data_sizes.items():
                            alert_details += f" â€¢ {file_type}ãƒ‡ãƒ¼ã‚¿: {size:.2f} KB\n"
                # ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã™ã‚‹å ´åˆã¯#local_testãƒãƒ£ãƒ³ãƒãƒ«ã«é€ä¿¡
                if not config.save_to_s3 and config.slack_webhook_url_local_test:
                    alert_manager.send_success_alert(
                        alert_message,
                        alert_details,
                        source="fetch_daily.py",
                        send_email=config.email_enabled,
                        send_slack=True,
                        additional_fields=common_fields + success_fields,
                        webhook_url=config.slack_webhook_url_local_test
                    )
                else:
                    alert_manager.send_success_alert(
                        alert_message,
                        alert_details,
                        source="fetch_daily.py",
                        send_email=config.email_enabled,
                        send_slack=True,
                        additional_fields=common_fields + success_fields
                    )
                logger.info("âœ… æˆåŠŸé€šçŸ¥ã‚’é€ä¿¡ã—ã¾ã—ãŸ")
        except Exception as e:
            logger.error(f"âŒ Slacké€šçŸ¥å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    else:
        logger.info("Slacké€šçŸ¥ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
    
    # Return exit code
    return 0 if failure_count == 0 else 1

if __name__ == "__main__":
    try:
        sys.exit(main())
    except Exception as e:
        # Catch any unexpected exceptions
        print(f"âŒ Critical error: {e}")
        traceback.print_exc()
        sys.exit(1)
