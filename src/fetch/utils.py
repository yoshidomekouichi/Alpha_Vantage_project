import os
import logging
import requests
import pandas as pd
import psycopg2
from pathlib import Path
from psycopg2.extras import execute_values
from dotenv import load_dotenv

# âœ… ç’°å¢ƒå¤‰æ•°ã®ãƒ­ãƒ¼ãƒ‰
dotenv_path = Path(__file__).parent.parent.parent / ".env"
if os.path.exists(dotenv_path):
    load_dotenv(dotenv_path)
else:
    raise FileNotFoundError(f"âš ï¸ .env ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {dotenv_path}")

# âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š
DB_CONFIG = {
    "dbname": "prod_db",
    "user": "myuser",
    "password": "0000",
    "host": "localhost",
    "port": "5434"  # Dockerå´ã®ãƒãƒ¼ãƒˆã«åˆã‚ã›ã‚‹ï¼
}

# âœ… ãƒ­ã‚°ã®è¨­å®š
class LoggerManager:
    def __init__(self, script_name):
        self.log_dir = Path(__file__).parent.parent.parent / "logs"
        os.makedirs(self.log_dir, exist_ok=True)

        log_file = self.log_dir / f"{script_name}.log"

        self.logger = logging.getLogger(script_name)
        self.logger.setLevel(logging.DEBUG)

        # âœ… æ—¢å­˜ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒãªã‘ã‚Œã°è¿½åŠ 
        if not self.logger.hasHandlers():
            file_handler = logging.FileHandler(log_file, mode='a')  # âœ… è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ã«ã™ã‚‹
            console_handler = logging.StreamHandler()

            formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")
            file_handler.setFormatter(formatter)
            console_handler.setFormatter(formatter)

            self.logger.addHandler(file_handler)
            self.logger.addHandler(console_handler)

        self.logger.propagate = False  # âœ… root ãƒ­ã‚¬ãƒ¼ã¸ã®ä¼æ’­ã‚’é˜²ã

    def get_logger(self):
        return self.logger

# âœ… APIãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•°ï¼ˆAPIä»•æ§˜å¤‰æ›´ã®ãƒã‚§ãƒƒã‚¯è¾¼ã¿ï¼‰
def fetch_stock_data(history=False):
    BASE_URL = 'https://www.alphavantage.co/query'
    SYMBOL = 'NVDA'
    API_KEY = os.getenv('ALPHA_VANTAGE_API_KEY')

    params = {
        "function": "TIME_SERIES_DAILY",
        "symbol": SYMBOL,
        "apikey": API_KEY,
        "datatype": "json"
    }

    if history:
        params["outputsize"] = "full"

    try:
        response = requests.get(BASE_URL, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        logging.debug(f"ğŸ›  APIãƒªã‚¯ã‚¨ã‚¹ãƒˆURL: {response.url}")
        if "Time Series (Daily)" not in data:
            raise ValueError(f"âŒ APIä»•æ§˜å¤‰æ›´ã®å¯èƒ½æ€§: 'Time Series (Daily)' ã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼\nãƒ¬ã‚¹ãƒãƒ³ã‚¹: {data}")

        sample_date = max(data["Time Series (Daily)"].keys())
        sample_data = data["Time Series (Daily)"][sample_date]

        expected_keys = {"1. open", "2. high", "3. low", "4. close", "5. volume"}
        actual_keys = set(sample_data.keys())

        if actual_keys != expected_keys:
            raise ValueError(f"âŒ APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›´ detectedï¼\næœŸå¾…: {expected_keys}\nå®Ÿéš›: {actual_keys}")

        return {"Time Series (Daily)": data["Time Series (Daily)"]} if history else {"Time Series (Daily)": {max(data["Time Series (Daily)"].keys()): data["Time Series (Daily)"][max(data["Time Series (Daily)"].keys())]}}

    except requests.exceptions.RequestException as e:
        logging.exception(f"âŒ API ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return None
    except ValueError as ve:
        logging.exception(f"âŒ APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ç•°å¸¸: {ve}")
        return None

# âœ… ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯é–¢æ•°
def check_data_quality(stock_data):
    time_series = stock_data.get("Time Series (Daily)", {})

    if not time_series:
        logging.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ï¼APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        return False

    df = pd.DataFrame.from_dict(time_series, orient="index")
    df = df.astype(float)  # ã“ã“ã§æ˜ç¤ºçš„ã«å¤‰æ›
    df.index = pd.to_datetime(df.index)
    df.columns = ["open", "high", "low", "close", "volume"]

    if not df.index.is_monotonic_decreasing:
        logging.warning("âš ï¸ æ—¥ä»˜ãŒé™é †ã«ãªã£ã¦ã„ã¾ã›ã‚“ï¼")
        return False
    if df.isnull().sum().sum() > 0:
        logging.warning(f"âš ï¸ æ¬ æå€¤ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ï¼\n{df.isnull().sum()}")
        return False
    if (df["volume"] == 0).any():
        logging.warning(f"âš ï¸ å‡ºæ¥é«˜ãŒ0ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã™ï¼\n{df[df['volume'] == 0]}")
        return False
    if (df["low"] < 0).any() or (df["high"] > df["high"].quantile(0.99) * 10).any():
        logging.warning("âš ï¸ ä¾¡æ ¼ã«æ¥µç«¯ãªå€¤ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ï¼")
        return False

    logging.info("âœ… ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ OKï¼")
    return True

# âœ… DBä¿å­˜é–¢æ•°
def save_to_db(stock_data):
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()

        time_series = stock_data.get("Time Series (Daily)", {})
        rows = [(date, float(data["1. open"]), float(data["2. high"]), float(data["3. low"]),
                 float(data["4. close"]), int(data["5. volume"]))
                for date, data in time_series.items()]

        insert_sql = """
        INSERT INTO test_stock_prices (date, open, high, low, close, volume)
        VALUES %s
        ON CONFLICT (date) DO UPDATE SET
        open = EXCLUDED.open,
        high = EXCLUDED.high,
        low = EXCLUDED.low,
        close = EXCLUDED.close,
        volume = EXCLUDED.volume;
        """
        execute_values(cur, insert_sql, rows)
        conn.commit()
        logging.info(f"âœ… {len(rows)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼")
    except Exception as e:
        logging.exception(f"âŒ ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        if conn:
            conn.close()
            logging.debug("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã¾ã—ãŸ")