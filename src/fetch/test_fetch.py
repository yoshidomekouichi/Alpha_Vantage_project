import os
import requests
import logging
import pandas as pd
import psycopg2
from psycopg2.extras import execute_values
from dotenv import load_dotenv



# âœ… ç’°å¢ƒå¤‰æ•°ã®ãƒ­ãƒ¼ãƒ‰
load_dotenv('/Users/koichi/Library/CloudStorage/GoogleDrive-yoshidome.kouichi@gmail.com/.shortcut-targets-by-id/1YPflACfNCBy1sNnuIrSAGpX5J8mL3Pcn/Google Drive/01.Private/Documents/Alpha_Vantage_project/configs/secrets/.env')
API_KEY = os.getenv('ALPHA_VANTAGE_API_KEY')
if not API_KEY:
    raise ValueError("APIã‚­ãƒ¼ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

# âœ… Alpha Vantage API è¨­å®š
BASE_URL = 'https://www.alphavantage.co/query'
SYMBOL = 'NVDA'
FUNCTION = 'TIME_SERIES_DAILY'

params = {
    "function": FUNCTION,
    "symbol": SYMBOL,
    "apikey": API_KEY,
    "outputsize": "compact",
    "datatype": "json"
}

# âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š

DB_CONFIG = {
    "dbname": "prod_db",
    "user": "myuser",
    "password": "0000",
    "host": "localhost",
    "port": "5434",  # Dockerå´ã®ãƒãƒ¼ãƒˆã«åˆã‚ã›ã‚‹ï¼
}


# âœ… ãƒ­ã‚°ã®è¨­å®š
def set_logger():
    log_dir = 'logs'
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, 'alpha_vantage_API_fetch.log')

    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)

    if not logger.handlers:
        console_handler = logging.StreamHandler()
        file_handler = logging.FileHandler(log_file)

        formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")
        console_handler.setFormatter(formatter)
        file_handler.setFormatter(formatter)

        logger.addHandler(console_handler)
        logger.addHandler(file_handler)
    
    return logger

logger = set_logger()

# âœ… API ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
def fetch_stock_data():
    logger.debug("ã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹")
    logger.debug(f"API_KEY: {API_KEY}")  # APIã‚­ãƒ¼ã®ç¢ºèª
    try:
        response = requests.get(BASE_URL, params=params, timeout=10)
        response.raise_for_status()

        data = response.json()
        logger.debug(f"API Response Status: {response.status_code}")
        logger.debug(f"API Response Content: {response.text[:500]}")  # 500æ–‡å­—ã ã‘å‡ºåŠ›
        logger.debug(f"API Request URL: {response.url}")  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆURLã®ç¢ºèª

        if "Time Series (Daily)" not in data:
            raise ValueError(f"Unexpected response format: {data}")
        return data
    except requests.exceptions.RequestException as e:
        logger.exception(f"API request failed: {e}")
        return None
    except ValueError as ve:
        logger.exception(f"Data format error: {ve}")
        return None

# âœ… ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
def check_data_quality(stock_data):
    time_series = stock_data.get("Time Series (Daily)", {})

    if not time_series:
        logger.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ï¼APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        return False

    df = pd.DataFrame.from_dict(time_series, orient="index", dtype="float")
    df.index = pd.to_datetime(df.index)
    df.columns = ["open", "high", "low", "close", "volume"]

    if not df.index.is_monotonic_decreasing:
        logger.warning("âš ï¸ æ—¥ä»˜ãŒé™é †ã«ãªã£ã¦ã„ã¾ã›ã‚“ï¼")
        return False
    if df.isnull().sum().sum() > 0:
        logger.warning(f"âš ï¸ æ¬ æå€¤ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ï¼\n{df.isnull().sum()}")
        return False
    if (df["volume"] == 0).any():
        logger.warning(f"âš ï¸ å‡ºæ¥é«˜ãŒ0ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã™ï¼\n{df[df['volume'] == 0]}")
        return False
    if (df["low"] < 0).any() or (df["high"] > df["high"].quantile(0.99) * 10).any():
        logger.warning("âš ï¸ ä¾¡æ ¼ã«æ¥µç«¯ãªå€¤ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ï¼")
        return False

    logger.info("âœ… ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ OKï¼")
    return True

def save_to_db(stock_data):
    conn = None
    try:
        logger.debug("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‹å§‹")
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()

        # ãƒ†ãƒ¼ãƒ–ãƒ«ãŒç„¡ã‘ã‚Œã°ä½œæˆ
        create_table_sql = """
        CREATE TABLE IF NOT EXISTS test_stock_prices (
            date DATE PRIMARY KEY,
            open NUMERIC(10,2),
            high NUMERIC(10,2),
            low NUMERIC(10,2),
            close NUMERIC(10,2),
            volume BIGINT
        );
        """
        cur.execute(create_table_sql)  # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆã‚’å®Ÿè¡Œ
        conn.commit()  # å¤‰æ›´ã‚’ç¢ºå®š

        time_series = stock_data.get("Time Series (Daily)", {})
        rows = [
            (date, float(data["1. open"]), float(data["2. high"]), float(data["3. low"]),
             float(data["4. close"]), int(data["5. volume"]))
            for date, data in time_series.items()
        ]

        insert_sql = """
        INSERT INTO test_stock_prices (date, open, high, low, close, volume)
        VALUES %s
        ON CONFLICT (date) DO UPDATE SET
        open = EXCLUDED.open,
        high = EXCLUDED.high,
        low = EXCLUDED.low,
        close = EXCLUDED.close,
        volume = EXCLUDED.volume;
        """
        execute_values(cur, insert_sql, rows)
        conn.commit()
        logger.info(f"âœ… {len(rows)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼")
    except Exception as e:
        logger.exception(f"âŒ ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        if conn:
            conn.close()
            logger.debug("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã¾ã—ãŸ")

# âœ… ãƒ¡ã‚¤ãƒ³å‡¦ç†
if __name__ == "__main__":
    logger.info("ğŸš€ ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’é–‹å§‹")

    stock_data = fetch_stock_data()

    if stock_data:
        is_valid = check_data_quality(stock_data)
        if is_valid:
            save_to_db(stock_data)

    logger.info("ğŸ‰ å‡¦ç†å®Œäº†")